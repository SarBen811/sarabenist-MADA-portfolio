[
  {
    "objectID": "fluanalysis/code/machinelearning.html",
    "href": "fluanalysis/code/machinelearning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "For the machine learning exercise, we will use the flu data and focus on Body Temperature as the outcome. We will be using the Decision Tree model, LASSO model, and Random forest model to determine the best performing model."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#load-packages",
    "href": "fluanalysis/code/machinelearning.html#load-packages",
    "title": "Machine Learning",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(rsample)\nlibrary(rpart.plot)\nlibrary(vip)\nlibrary(glmnet)\nlibrary(ranger)\nlibrary(ggplot2)\nlibrary(ggfortify)"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#load-data",
    "href": "fluanalysis/code/machinelearning.html#load-data",
    "title": "Machine Learning",
    "section": "Load data",
    "text": "Load data\n\n#load data\nfilelocation <- here(\"fluanalysis\", \"data\", \"cleandata.rds\")\nload(filelocation)\n\n#reassign as flu\nflu <- cleandata\n\n#remove columns with <50 entries in one category\nflu <- flu %>% \n  select(!c(\"Vision\",\"Hearing\"))"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#data-splitting",
    "href": "fluanalysis/code/machinelearning.html#data-splitting",
    "title": "Machine Learning",
    "section": "Data splitting",
    "text": "Data splitting\n\n#establish reproducibility by setting the seed\nset.seed(123)\n\n#stratify data split to balance outcomes between data sets\ndata_split1 <- initial_split(flu, prop = 7/10, strata = BodyTemp)\n\n#create two data sets with 70% of data in training set\ntrain_data1 <- training(data_split1)\ntest_data1 <- testing(data_split1)"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#cross-validation-workflow-set-up",
    "href": "fluanalysis/code/machinelearning.html#cross-validation-workflow-set-up",
    "title": "Machine Learning",
    "section": "Cross Validation & Workflow set up",
    "text": "Cross Validation & Workflow set up\n\n#set seed for reproducibility\nset.seed(123)\nfolds <- vfold_cv(train_data1, v = 5)\n\n#create recipe for machine learning\n#step_ functions create dummy variables and order the severity columns\nml_rec <- recipe(BodyTemp ~., data = train_data1) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_ordinalscore()\n\n#set linear regression\nlm_mod <- linear_reg() \n\n#create machine learning workflow\nml_wf <- workflow() %>% \n  add_model(lm_mod) %>% \n  add_recipe(ml_rec)"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#null-model",
    "href": "fluanalysis/code/machinelearning.html#null-model",
    "title": "Machine Learning",
    "section": "Null model",
    "text": "Null model\nWe will use the null model as comparison for the other models. All other models should perform better than the null model since this model does not use any of the predictor variables.\n\n#set null model using null_model() function\nnullmod <- null_model() %>% \n  set_engine(\"parsnip\") %>% \n  set_mode(\"regression\")\n\n#create null recipe\nnull_rec <- recipe(BodyTemp ~ ., data = train_data1)%>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_ordinalscore()\n\n#create null workflow\nnullmodwf <- workflow() %>% \n  add_model(nullmod) %>% \n  add_recipe(null_rec)\n\n#fit the null model to training data\nnullmodfit <- nullmodwf %>% \n  fit(data = train_data1)\n\nThe null model can be used to make predictions of the outcome, but without any predictors, the model will produce the mean body temperature for the training and test data. The rmse() function will produce the metric for this model.\n\n#use null model to make predictions from training data\nnullmod_predtrain <- augment(nullmodfit, train_data1) %>% \n  rmse(truth = BodyTemp, .pred)\nnullmod_predtrain\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.21\n\n\n\n#use null model to make predictions from test data\nnullmod_predtest <-augment(nullmodfit, test_data1) %>% \n  rmse(BodyTemp, .pred)\nnullmod_predtest\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.16\n\n\nThe training null model has a RMSE of 1.209 and the test null model performs slightly better with a RMSE of 1.163."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#model-tuning-and-fitting",
    "href": "fluanalysis/code/machinelearning.html#model-tuning-and-fitting",
    "title": "Machine Learning",
    "section": "Model tuning and fitting",
    "text": "Model tuning and fitting\nFor each model, we will use the tune() function to find the best parameters during the cross-validation step. After finding the best model based on our performance metric RMSE, we will fit the best model.\n\nTree model specification\n\n#set up decision tree model and tune the parameters\ntune_spec <- decision_tree(\n  cost_complexity = tune(), #one parameter we will tune\n  tree_depth = tune() #another parameter\n) %>% \n  set_engine(\"rpart\") %>% #specify engine\n  set_mode(\"regression\") #regression due to continuous outcome\n\n\n\nTree model workflow\n\n#create decision tree workflow\ntree_wf <- workflow() %>% \n  add_model(tune_spec) %>% \n  add_recipe(ml_rec) #can use the general machine learning recipe\n\n\n\nTree model grid specification\nThe grid_regular() will produce values of the parameters that we test to find the best model. For each value of tree depth, 5 cost_complexity values will be tested.\n\n#create grid of resampled values for cross-validation\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5) #produces 5x5 = 25 combinations\n\n\n\nTree model tuning with cross-validation\nCross-validation is required for tuning models. In order to find the best model, we can use resampled data from the training set to test the models against “new data” in order to see the variation within the model.\n\nset.seed(123)\ntree_res <- tree_wf %>% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control = control_grid(save_pred = TRUE)\n  )\n\n! Fold1: internal: A correlation computation is required, but `estimate` is constant and ha...\n\n\n! Fold2: internal: A correlation computation is required, but `estimate` is constant and ha...\n\n\n! Fold3: internal: A correlation computation is required, but `estimate` is constant and ha...\n\n\n! Fold4: internal: A correlation computation is required, but `estimate` is constant and ha...\n\n\n! Fold5: internal: A correlation computation is required, but `estimate` is constant and ha...\n\n\nPlotting the resampled models produces a summary of the RMSE in the top row. A lower RMSE is preferred.\n\ntree_res %>%\n  autoplot()\n\n\n\n\nIt would appear that the model with tree_depth = 1 has the best RMSE. ### Tree model performance To confirm the best performing tree model, we can use select_best() function and extract the metrics.\n\n#select best tree model\nbest_tree <- tree_res %>% select_best(\"rmse\")\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n\n#set up workflow\ntreefinal_wf <- tree_wf %>% \n  finalize_workflow(best_tree)\n\n#fit the final model\ntreefinal_fit <- treefinal_wf %>% \n  fit(train_data1) \n\n#extract RMSE value\ntreefinalfitted_rmse<- augment(treefinal_fit, train_data1) %>% \n  select(BodyTemp, .pred) %>% \n  rmse(truth = BodyTemp, .pred)\n\nThe best performing tree model has a tree depth of 1 and and RMSE of 1.178 which is slightly better than the training null model of 1.209.\n\n#save for later comparison: RMSE based on predictions from best model\ntree_rmse <- tree_res %>% \n  collect_predictions(parameters = best_tree) %>% \n  rmse(BodyTemp, .pred) %>% \n  mutate(model = \"Tree\")\ntree_rmse\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate model\n  <chr>   <chr>          <dbl> <chr>\n1 rmse    standard        1.19 Tree \n\n\nThe decision tree can be plotted using rpart.plot(). The level of importance for each predictor can be found using vip() function.\n\n#extract engine and plot decision tree\ntreefinal_fit%>% \n  extract_fit_engine() %>% \n  rpart.plot(roundint = FALSE)\n\n\n\n#plot importance of predictors\ntreefinal_fit %>% \n  extract_fit_parsnip() %>% \n  vip()\n\n\n\n\n\n\nPlots for Tree model\nWe may also be interested in the predictions and residuals for the model compared to the actual outcomes. First, we need to create the predictions using augment() and the residuals using mutate().\n\n#create predictions and residuals\ntree_resd <- augment(treefinal_fit, train_data1) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n\n#plot body temperature and predictions\ntreeplot1 <- ggplot(tree_resd)+\n  geom_point(aes(BodyTemp, .pred))+\n  scale_x_continuous(limits = c(97,103.5))+\n  scale_y_continuous(limits = c(97,103.5))\ntreeplot1\n\n\n\n\n\n#plot body temperature and residuals\ntreeplot2 <- ggplot(tree_resd)+\n  geom_point(aes(BodyTemp,.resid))\ntreeplot2\n\n\n\n\nThe predictions and residuals show that the models do not perform well in predicting body temperature.\n\n\nCompare model to null\nAs mentioned before, the tree model is only slightly better than the null model (RMSE of 1.88 (SE = 0.061) compared to 1.209). The fitted tree model has a RMSE of 1.178.\n\n#metrics for best model\ntree_res %>% show_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n1    0.0000000001          1 rmse    standard    1.19     5  0.0613 Preprocesso…\n2    0.0000000178          1 rmse    standard    1.19     5  0.0613 Preprocesso…\n3    0.00000316            1 rmse    standard    1.19     5  0.0613 Preprocesso…\n4    0.000562              1 rmse    standard    1.19     5  0.0613 Preprocesso…\n5    0.0000000001          4 rmse    standard    1.20     5  0.0504 Preprocesso…\n\ntreefinalfitted_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.18\n\n\n\n\nLASSO model specifications\nNext we will look at the LASSO model following the same general steps as the decision tree model.\n\n#set LASSO model\nlm_modLASSO <- linear_reg(penalty = tune(), #penalties given for number of predictors\n                          mixture = 1) %>% \n  set_engine(\"glmnet\")\n\n\n\nLASSO model workflow\n\n#set LASSO workflow\nLASS_wf <- workflow() %>% \n  add_model(lm_modLASSO) %>% \n  add_recipe(ml_rec)\n\n\n\nLASSO model grid specifications\nWe will use a similar grid method as before in order to tune the parameters of the models.\n\n#create LASSO grid\nLASS_grid <-tibble(penalty = 10^seq(-4, -1, length.out = 30)) #creates a grid of penalty values to tune\nLASS_grid %>% top_n(-5) #lowest penalty values\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\n\nPenalties will be highest for models with the most predictors.\n\n\nLASSO model tuning with cross-validation\n\nset.seed(123)\nLASS_res <-\n  LASS_wf %>% \n  tune_grid(folds, #cross-validation\n            grid = LASS_grid, #penalty grid\n            control = control_grid(save_pred = TRUE), #save CV predictions to use later\n            metrics = metric_set(rmse)) #RMSE as metric\n\nWe can plot the metrics against penalty values.\n\nLASS_plot <- \n  LASS_res %>% \n  autoplot()\n\nLASS_plot \n\n\n\n\nWe would like the lowest RMSE and the lowest penalty, so the best model should have an RMSE around 1.15.\n\n\nLASSO model performance\nUsing select_best() function and extracting the metrics will give us the model performance measure.\n\n#show top performing models\nLASS_showbest <- LASS_res %>% show_best()\n\n#select best model\nbest_LASS_model <- LASS_res %>% \n  select_best(\"rmse\")\nbest_LASS_model\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0386 Preprocessor1_Model26\n\n#update workflow with best model\nLASSfinal_wf <- LASS_wf %>% \n  finalize_workflow(best_LASS_model)\n\n#fit the model to the training data\nLASSfinal_fit <- LASSfinal_wf %>% \n  fit(train_data1) \n\n#collect final RMSE\nLASSfinalfitted_rmse<- augment(LASSfinal_fit, train_data1) %>% \n  select(BodyTemp, .pred) %>% \n  rmse(truth = BodyTemp, .pred)\n\nThe LASSO model (RMSE = 1.117) performed better than the decision tree model (1.187) and the null model (1.209).\n\n#save for later comparison: RMSE based on predictions from best model\nLASS_rmse <- LASS_res %>% \n  collect_predictions(parameters = best_LASS_model) %>% \n  rmse(BodyTemp, .pred) %>% \n  mutate(model = \"LASSO\")\nLASS_rmse\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate model\n  <chr>   <chr>          <dbl> <chr>\n1 rmse    standard        1.15 LASSO\n\n\n\n\nPlots for LASSO model\nWe can again plot the predictions and residuals against the outcome.\n\n#create predictions and residuals\nLASS_resd <- augment(LASSfinal_fit, train_data1) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n\n\n#plots LASSO predictions and body temperature\nLASS_plot1 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp, .pred))+\n  scale_x_continuous(limits = c(97,103.5))+\n  scale_y_continuous(limits = c(97,103.5))\nLASS_plot1\n\n\n\n\n\n#plots residuals and body temperature\nLASS_plot2 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp,.resid))\nLASS_plot2\n\n\n\n\nThe prediction and residual plots show a better performance than the other two models we have looked at so far.\n\n\nCompare model to null model\nAs mentioned, the LASSO model performs better than the null mode with a RMSE of 1.148 (SE = 0.0534) compared the null model (1.209). The fitted LASSO model has an RMSE of 1.117.\n\nLASS_res %>% \n  show_best() \n\n# A tibble: 5 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0386 rmse    standard    1.15     5  0.0534 Preprocessor1_Model26\n2  0.0489 rmse    standard    1.15     5  0.0542 Preprocessor1_Model27\n3  0.0304 rmse    standard    1.15     5  0.0528 Preprocessor1_Model25\n4  0.0621 rmse    standard    1.15     5  0.0552 Preprocessor1_Model28\n5  0.0240 rmse    standard    1.15     5  0.0524 Preprocessor1_Model24\n\nLASSfinalfitted_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.12\n\n\n\n\nRandom forest model specifications\nFinally, following the same steps are above, we can look at random forest models using cross-validation and tuning to find the best model.\n\n#how many cores your computer can use for running tuning on multiple models at the same time \ncores <- parallel::detectCores()\n\n#set up random forest model\nrf_mod <- rand_forest(mtry = tune(), #parameter to tune\n                      min_n = tune(), #parameter to tune\n                      trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% \n  set_mode(\"regression\")\n\n\n\nRandom forest model workflow\n\n#create random forest recipe\nrf_rec <- recipe(BodyTemp ~ ., data = train_data1) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_ordinalscore()\n#create workflow\nrf_wf <- workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(rf_rec)\n\n\n\nRandom forest model grid specifications/tuning with cross-validation\nWe will use a similar grid method for tuning model parameters\n\n#create grid for tuning parameters for cross-validation\nset.seed(123)\nrf_res <- rf_wf %>% \n  tune_grid(folds,\n            grid = 25, #25 models\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can look at the best random forest models by RMSE values.\n\n#show top performing models\nrf_showbest <- rf_res %>% show_best()\n\nrf_showbest\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8    37 rmse    standard    1.16     5  0.0574 Preprocessor1_Model25\n2     4    14 rmse    standard    1.16     5  0.0589 Preprocessor1_Model06\n3     3    23 rmse    standard    1.16     5  0.0603 Preprocessor1_Model13\n4    13    35 rmse    standard    1.16     5  0.0564 Preprocessor1_Model15\n5     7    12 rmse    standard    1.17     5  0.0559 Preprocessor1_Model18\n\n\nThe best performing model has mtry (number of predictors at each node) = 8, min_n (min number of data points required to split) = 37 and RMSE of 1.158 (SE = 0.057). This is a better performing model compared to the null and other models.\nThe autoplot() function visually shows the performance of the various models.\n\nrf_res %>% autoplot()\n\n\n\n\nWe will extract the best model from the group using select_best().\n\n#extract best model\nrf_best <-\n  rf_res %>% \n  select_best(metric = \"rmse\")\n#show results\nrf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     8    37 Preprocessor1_Model25\n\n\n\n#collect predictions of best random forest model and calculate RMSE\n#save for later comparison: RMSE based on predictions from best model\nrf_rmse <-\n  rf_res %>% \n  collect_predictions(parameters = rf_best) %>% \n  rmse(truth = BodyTemp, estimate = .pred) %>% \n  mutate(model = \"Random Forest\")\nrf_rmse\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate model        \n  <chr>   <chr>          <dbl> <chr>        \n1 rmse    standard        1.16 Random Forest\n\n\nUsing the best random forest model, the RMSE is 1.163 which is similar to the LASSO model.\n\n\nFinal fit for random forest model\nNow we can fit the model to the training data to get the final RMSE value.\n\n#set random forest model using best model parameters\nrffinal_mod <-\n  rand_forest(mtry = 8, min_n = 37, trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") %>% \n  set_mode(\"regression\")\n#update workflow\nrffinal_wf <- rf_wf %>% \n  update_model(rffinal_mod)\n#fit to training data\nrffinal_fit <- rffinal_wf %>%\n  fit(train_data1)\n#pull RMSE from fitted model\nrffinalfitted_rmse <- augment(rffinal_fit, train_data1) %>% \n  select(BodyTemp, .pred) %>% \n  rmse(truth = BodyTemp, .pred)\n\nThe fitted random forest model RMSE is 1.028 which is slightly better than the LASSO model and better than the null and decision tree models.\nWe can also look at the important predictors for the random forest model using vip() function`\n\nrffinal_fit %>% \n  extract_fit_parsnip() %>% \n  vip(num_features = 20)\n\n\n\n\nSneezing remains as the most important predictor for body temperature followed closely by subjective fever.\n\n\nPlots for Random forest model\nWe will again plot the predictions and residuals for the model using augment().\n\n#create predictions and residuals\nrf_resd <- augment(rffinal_fit, train_data1) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n\n#plot body temperature and predictions\nrf_plot1 <- ggplot(rf_resd)+\n  geom_point(aes(BodyTemp,.pred))\nrf_plot1\n\n\n\n\n\n#plot body temperature and residuals\nrf_plot2 <- ggplot(rf_resd)+\n  geom_point(aes(BodyTemp, .resid))\nrf_plot2\n\n\n\n\nThe residuals and prediction plots look similar to the LASSO model.\n\n\nCompare model to null model\n\nrf_res %>% show_best() %>% arrange(mean) %>% top_n(1)\n\nSelecting by .config\n\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8    37 rmse    standard    1.16     5  0.0574 Preprocessor1_Model25\n\n\nCompared to the null, the random forest model is a better predictor of body temperature with a RMSE of 1.158 (SE = 0.057) compared to 1.209."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#choosing-best-model",
    "href": "fluanalysis/code/machinelearning.html#choosing-best-model",
    "title": "Machine Learning",
    "section": "Choosing best model",
    "text": "Choosing best model\nNow that we have looked at all of the models, we can choose the best model to test with our test_data1 data set which has not been used during the tuning/fitting process.\n\n#create comparison tibble for three models\ncompare <- bind_rows(tree_rmse,LASS_rmse,rf_rmse)\ncompare\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate model        \n  <chr>   <chr>          <dbl> <chr>        \n1 rmse    standard        1.19 Tree         \n2 rmse    standard        1.15 LASSO        \n3 rmse    standard        1.16 Random Forest\n\n\nBased on the RMSE metric, the best performing models are the LASSO and Random forest models. We can also check the uncertainty by looking at the plots shown above and the standard error.\n\nLASS_showbest %>% head(1)\n\n# A tibble: 1 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0386 rmse    standard    1.15     5  0.0534 Preprocessor1_Model26\n\nrf_showbest %>% head(1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8    37 rmse    standard    1.16     5  0.0574 Preprocessor1_Model25\n\n\nSince the LASSO model has a lower SE than the random forest model, we will choose the LASSO model for testing the model on new data."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#final-evaluation",
    "href": "fluanalysis/code/machinelearning.html#final-evaluation",
    "title": "Machine Learning",
    "section": "Final evaluation",
    "text": "Final evaluation\nNow we can fit the LASSO model to the test data to check the RMSE on “new data”.\n\nLASStest_fit <-\n  LASSfinal_wf %>% \n  last_fit(data_split1)\nLASStest_fit %>% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      1.16   Preprocessor1_Model1\n2 rsq     standard      0.0297 Preprocessor1_Model1\n\n\nWith the test data, the model has a RMSE of 1.16 which is slightly worse than the fitted model. This would make sense as the fitted model is designed off the train_data1 set.\nWe will plot the same prediction and residual plots as before using the test_data1 outcomes.\n\n#create predictions and residuals for test data\nLASStest_resd <- augment(LASStest_fit) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n\n\n#plots LASSO predictions and body temperature for test data\nLASS_plot3 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp, .pred))+\n  scale_x_continuous(limits = c(97,103.5))+\n  scale_y_continuous(limits = c(97,103.5))\nLASS_plot3\n\n\n\n\n\n#plots residuals and body temperature for test data\nLASS_plot4 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp,.resid))\nLASS_plot4"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "Model Evaluation",
    "section": "",
    "text": "This script will use the flu analysis data to evaluate the model predicting the main categorical outcome Nausea and main continuous outcome BodyTemp."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#load-packages",
    "href": "fluanalysis/code/modeleval.html#load-packages",
    "title": "Model Evaluation",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(here)\nlibrary(rsample)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#load-data",
    "href": "fluanalysis/code/modeleval.html#load-data",
    "title": "Model Evaluation",
    "section": "Load data",
    "text": "Load data\n\n#load data\nfilelocation <- here(\"fluanalysis\", \"data\", \"cleandata.rds\")\nload(filelocation)\n#reassign as flu\nflu <- cleandata\n\n\nflu <- flu %>% \n  select(!c(\"Vision\",\"Hearing\")) #remove columns with <50 entries in one category"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#data-splitting",
    "href": "fluanalysis/code/modeleval.html#data-splitting",
    "title": "Model Evaluation",
    "section": "Data splitting",
    "text": "Data splitting\nTo start model evaluation, we will first split the data into training and testing sets.\n\n#establish reproducibility by setting the seed\nset.seed(123)\n\n#add data to the training set\ndata_split <- initial_split(flu, prop = 3/4)\n\n#create two data sets with 3/4 of data in training set\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-a-recipe-and-workflow-from-all-symptoms",
    "href": "fluanalysis/code/modeleval.html#create-a-recipe-and-workflow-from-all-symptoms",
    "title": "Model Evaluation",
    "section": "Create a recipe and workflow from all symptoms",
    "text": "Create a recipe and workflow from all symptoms\nWe will first create a recipe for a logistic regression model predicting nausea from all predictor variables. The recipe uses the recipe() function and will contain the formula and the data (the training set).\n\nflu_rec <- \n  recipe(Nausea ~ . , data = train_data) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n\nNext we will set a model workflow to pair the model and recipe together. This will help when evaluating model based on the training and testing data set.\n\n#set model\nlr_mod <- logistic_reg() %>% \n  set_engine(\"glm\")\n\nflu_wflow <-\n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_rec)\n\nWe can create one function that will create the recipe and train the model using the workflow() and fit() functions.\n\nflu_fit <-\n  flu_wflow %>% \n  fit(data = train_data)\n\nTo check the fitted model, the extract_fit_parsnip() function will display the fitted model.\n\nflu_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 32 × 5\n   term                  estimate std.error statistic p.value\n   <chr>                    <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)            -2.83      9.07      -0.312  0.755 \n 2 BodyTemp               -0.0101    0.0921    -0.109  0.913 \n 3 SwollenLymphNodes_Yes  -0.435     0.235     -1.85   0.0649\n 4 ChestCongestion_Yes     0.277     0.252      1.10   0.271 \n 5 ChillsSweats_Yes        0.333     0.355      0.939  0.348 \n 6 NasalCongestion_Yes     0.233     0.300      0.778  0.437 \n 7 Sneeze_Yes              0.115     0.253      0.455  0.649 \n 8 Fatigue_Yes             0.288     0.460      0.626  0.531 \n 9 SubjectiveFever_Yes     0.402     0.271      1.48   0.138 \n10 Headache_Yes            0.644     0.367      1.75   0.0794\n# … with 22 more rows"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#predict-from-trained-model",
    "href": "fluanalysis/code/modeleval.html#predict-from-trained-model",
    "title": "Model Evaluation",
    "section": "Predict from trained model",
    "text": "Predict from trained model\nNext, we can use the test_data set to predict from the trained model by using the flu_fit.\n\npredict(flu_fit, test_data)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 Yes        \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 Yes        \n# … with 173 more rows\n\n\nThis output is not terribly helpful. The predicted probability of having nausea can be found by using the augment() function.\n\nflu_aug <- augment(flu_fit, test_data)\n\nTo evaluate the performance of the model, we will use the ROC curve and ROC-AUC as the metrics. Ideally, the model should have at least a value of 0.7 to be useful.\n\nlevels(flu$Nausea)\n\n[1] \"No\"  \"Yes\"\n\n#generate ROC curve\nflu_aug %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>% #must specify \"second\" since the positive event is \"Yes\" which is the second level\n  autoplot()\n\n\n\n#generate ROC-AUC\nflu_aug %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.709\n\n\nThe area under the ROC curve is 0.706 which indicates the model is somewhat useful.\nWe can also use the train_data to predict the from the model.\n\n#predict from training data\nflu_aug2 <- augment(flu_fit, train_data)\n\n#generate ROC curve\nflu_aug2 %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>%\n  autoplot()\n\n\n\n#generate ROC-AUC\nflu_aug2 %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.798\n\n\nThe ROC-AUC is higher with the train_data than the test_data at 0.80 which is understandable since the model was fitted to the train_data set."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-recipe-with-runny-nose-as-predictor",
    "href": "fluanalysis/code/modeleval.html#create-recipe-with-runny-nose-as-predictor",
    "title": "Model Evaluation",
    "section": "Create recipe with runny nose as predictor",
    "text": "Create recipe with runny nose as predictor\nUsing all the same steps as above, we can predict nausea from runny nose.\n\n#create recipe\nflu_recRN <- \n  recipe(Nausea ~ RunnyNose, data = train_data)%>%\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n\n#create work flow\nflu_wflowRN <-\n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_recRN)\n\n#create fitted model\nflu_fitRN <-\n  flu_wflowRN %>% \n  fit(data = train_data)\n\nPredicting outcome from the RunnyNose fitted model.\n\n#create predictions\nflu_augRN <- augment(flu_fitRN, test_data)\n\n#generate ROC curve\nflu_augRN %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>%\n  autoplot()\n\n\n\n#generate ROC-AUC\nflu_augRN %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.460\n\n\nUsing the test_data and RunnyNose as the predictor, the ROC-AUC is 0.460, indicating that the model is not helpful in predicting nausea.\nEvaluate the fitted model using the train_data.\n\n#predict from training data\nflu_augRN2 <- augment(flu_fit, train_data)\n\n#generate ROC curve\nflu_augRN2 %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>%\n  autoplot()\n\n\n\n#generate ROC-AUC\nflu_augRN2 %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.798\n\n\nUsing the train_data, the ROC-AUC is 0.801 which is much different from the test_data evaluation. This is possibly due to the random distribution of the data, with the train_data having a higher correlation between RunnyNose and Nausea observations. This is a good example of why fitted models should not be evaluated using only the data used to fit the model."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-workflow-for-a-linear-regression",
    "href": "fluanalysis/code/modeleval.html#create-workflow-for-a-linear-regression",
    "title": "Model Evaluation",
    "section": "Create workflow for a linear regression",
    "text": "Create workflow for a linear regression\nThis will be used to predict body temp from all other variables\n\nlm_mod <- linear_reg() #define model\n\nrecipe_bodytemp <- recipe(BodyTemp ~ ., data = train_data)%>% #set recipe to predict body temp using all variables\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n\nbodytemp_lm_workflow <- workflow() %>% #combine model and recipe to make workflow\n  add_model(lm_mod) %>% \n  add_recipe(recipe_bodytemp)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#data-fitting",
    "href": "fluanalysis/code/modeleval.html#data-fitting",
    "title": "Model Evaluation",
    "section": "Data fitting",
    "text": "Data fitting\n\nset.seed(626)\nbodytemp_fit <- bodytemp_lm_workflow %>% \n  fit(data = train_data)\ntidy(bodytemp_fit)\n\n# A tibble: 32 × 5\n   term                  estimate std.error statistic   p.value\n   <chr>                    <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)            98.2        0.352   279.    0        \n 2 SwollenLymphNodes_Yes  -0.104      0.108    -0.960 0.337    \n 3 ChestCongestion_Yes     0.0566     0.114     0.496 0.620    \n 4 ChillsSweats_Yes        0.279      0.151     1.84  0.0664   \n 5 NasalCongestion_Yes    -0.213      0.133    -1.60  0.110    \n 6 Sneeze_Yes             -0.400      0.116    -3.44  0.000627 \n 7 Fatigue_Yes             0.375      0.187     2.00  0.0461   \n 8 SubjectiveFever_Yes     0.517      0.122     4.25  0.0000259\n 9 Headache_Yes           -0.0614     0.151    -0.406 0.685    \n10 Weakness_Mild          -0.0473     0.226    -0.209 0.834    \n# … with 22 more rows"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-evaluation-on-training-data",
    "href": "fluanalysis/code/modeleval.html#model-evaluation-on-training-data",
    "title": "Model Evaluation",
    "section": "Model evaluation on training data",
    "text": "Model evaluation on training data\n\nbodytemp_aug <- augment(bodytemp_fit, train_data)\nbodytemp_aug %>% select(BodyTemp, .pred)\n\n# A tibble: 547 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.5  99.4\n 2     98.6  98.5\n 3     98.5  99.0\n 4    101.   99.5\n 5     98.5  97.8\n 6    100.   99.3\n 7     98.3  98.6\n 8     98.7  98.6\n 9     98.5  99.2\n10    102.   99.2\n# … with 537 more rows\n\nbodytemp_aug %>% \n  rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n\nbodytemp_aug %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n\n\n\n\nLooking at the plot, there doesn’t appear to be a strong relationship here."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-evaluation-on-testing-data",
    "href": "fluanalysis/code/modeleval.html#model-evaluation-on-testing-data",
    "title": "Model Evaluation",
    "section": "Model evaluation on testing data",
    "text": "Model evaluation on testing data\n\nbodytemp_aug_2 <- augment(bodytemp_fit, test_data)\nbodytemp_aug_2 %>% select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.5\n 2    101.   98.9\n 3     98.8  99.0\n 4     98.5  98.7\n 5     98.1  98.5\n 6     98.4  99.0\n 7     99.5  99.5\n 8     98.8  99.7\n 9    102.   98.9\n10     99.7  99.6\n# … with 173 more rows\n\nbodytemp_aug_2 %>% \n  rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.14\n\nbodytemp_aug_2 %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n\n\n\n\nNot seeing a strong relationship in the testing data either"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-workflow-for-linear-regression-pt.-2",
    "href": "fluanalysis/code/modeleval.html#create-workflow-for-linear-regression-pt.-2",
    "title": "Model Evaluation",
    "section": "Create workflow for linear regression pt. 2",
    "text": "Create workflow for linear regression pt. 2\nThis will be used to predict body temp from runny nose data\n\nlm_mod <- linear_reg() #define model\n\nrecipe_bodytemp2 <- recipe(BodyTemp ~ RunnyNose, data = train_data) %>%  #set recipe to predict body temp using runny nose %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n\nbodytemp_lm_workflow2 <- workflow() %>% #combine model and recipe to make workflow\n  add_model(lm_mod) %>% \n  add_recipe(recipe_bodytemp2)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#data-fitting-1",
    "href": "fluanalysis/code/modeleval.html#data-fitting-1",
    "title": "Model Evaluation",
    "section": "Data fitting",
    "text": "Data fitting\n\nset.seed(626)\nbodytemp_fit <- bodytemp_lm_workflow %>% \n  fit(data = train_data)\ntidy(bodytemp_fit)\n\n# A tibble: 32 × 5\n   term                  estimate std.error statistic   p.value\n   <chr>                    <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)            98.2        0.352   279.    0        \n 2 SwollenLymphNodes_Yes  -0.104      0.108    -0.960 0.337    \n 3 ChestCongestion_Yes     0.0566     0.114     0.496 0.620    \n 4 ChillsSweats_Yes        0.279      0.151     1.84  0.0664   \n 5 NasalCongestion_Yes    -0.213      0.133    -1.60  0.110    \n 6 Sneeze_Yes             -0.400      0.116    -3.44  0.000627 \n 7 Fatigue_Yes             0.375      0.187     2.00  0.0461   \n 8 SubjectiveFever_Yes     0.517      0.122     4.25  0.0000259\n 9 Headache_Yes           -0.0614     0.151    -0.406 0.685    \n10 Weakness_Mild          -0.0473     0.226    -0.209 0.834    \n# … with 22 more rows"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-evaluation-on-training-data-1",
    "href": "fluanalysis/code/modeleval.html#model-evaluation-on-training-data-1",
    "title": "Model Evaluation",
    "section": "Model evaluation on training data",
    "text": "Model evaluation on training data\n\nbodytemp_aug3 <- augment(bodytemp_fit, train_data)\nbodytemp_aug3 %>% select(BodyTemp, .pred)\n\n# A tibble: 547 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.5  99.4\n 2     98.6  98.5\n 3     98.5  99.0\n 4    101.   99.5\n 5     98.5  97.8\n 6    100.   99.3\n 7     98.3  98.6\n 8     98.7  98.6\n 9     98.5  99.2\n10    102.   99.2\n# … with 537 more rows\n\nbodytemp_aug3 %>% \n  rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n\nbodytemp_aug3 %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n\n\n\n\nThere is a slight positive correlation but not really."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-evaluation-on-testing-data-1",
    "href": "fluanalysis/code/modeleval.html#model-evaluation-on-testing-data-1",
    "title": "Model Evaluation",
    "section": "Model evaluation on testing data",
    "text": "Model evaluation on testing data\n\nbodytemp_aug_4 <- augment(bodytemp_fit, test_data)\nbodytemp_aug_4 %>% select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.5\n 2    101.   98.9\n 3     98.8  99.0\n 4     98.5  98.7\n 5     98.1  98.5\n 6     98.4  99.0\n 7     99.5  99.5\n 8     98.8  99.7\n 9    102.   98.9\n10     99.7  99.6\n# … with 173 more rows\n\nbodytemp_aug_4 %>% \n  rmse(truth = BodyTemp, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.14\n\nbodytemp_aug_4 %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n\n\n\n\nNo correlation\nOverall, I don’t think the flu symptom is very predictive of body temperature."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "This code script uses the data from this paper: McKay, Brian et al. (2020), Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of patients infected with influenza, Dryad, Dataset, https://doi.org/10.5061/dryad.51c59zw4v. The raw data has been placed in the data file of this repository."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#load-packages",
    "href": "fluanalysis/code/wrangling.html#load-packages",
    "title": "Wrangling",
    "section": "Load packages",
    "text": "Load packages\n\n# | output: false\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(readr) # for reading in files\nlibrary(here) # setting directory\n\nWarning: package 'here' was built under R version 4.2.2\n\n\nhere() starts at C:/Users/Sara/Documents/Documents/Current Classes/MADA/sarabenist-MADA-portfolio\n\nlibrary(naniar) # for finding missing data"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#load-data",
    "href": "fluanalysis/code/wrangling.html#load-data",
    "title": "Wrangling",
    "section": "Load data",
    "text": "Load data\nFirst, we will load the raw data.\n\n#load data from data folder\ndata <- readRDS(here(\"fluanalysis/data/SympAct_Any_Pos.rda\"))"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#data-exploring-and-cleaning",
    "href": "fluanalysis/code/wrangling.html#data-exploring-and-cleaning",
    "title": "Wrangling",
    "section": "Data exploring and cleaning",
    "text": "Data exploring and cleaning\nA quick overview of the imported data using the summary() function.\n\n#overview of data\nsummary(data)\n\n                                           DxName1   \n Influenza like illness - Clinical Dx          :328  \n Influenza - Virus Identified                  :131  \n Fever, unspecified                            :101  \n Cough                                         : 66  \n Acute pharyngitis, unspecified                : 50  \n Acute upper respiratory infection, unspecified: 22  \n (Other)                                       : 37  \n                                 DxName2   \n Influenza - Virus Identified        :126  \n Influenza like illness - Clinical Dx:115  \n Fever, unspecified                  : 45  \n Cough                               : 41  \n Acute pharyngitis, unspecified      : 31  \n (Other)                             : 97  \n NA's                                :280  \n                                 DxName3   \n Influenza - Virus Identified        : 23  \n Influenza like illness - Clinical Dx: 14  \n Cough                               : 10  \n Fever, unspecified                  :  6  \n Acute pharyngitis, unspecified      :  4  \n (Other)                             : 52  \n NA's                                :626  \n                                           DxName4   \n Influenza - Virus Identified                  :  3  \n Acute upper respiratory infection, unspecified:  2  \n Encounter for immunization                    :  2  \n Influenza like illness - Clinical Dx          :  2  \n Acute pharyngitis, unspecified                :  1  \n (Other)                                       :  9  \n NA's                                          :716  \n                                                                                               DxName5   \n Acute suppurative otitis media without spontaneous rupture of ear drum, right ear                 :  0  \n Encounter for immunization                                                                        :  0  \n Headache                                                                                          :  1  \n Other infectious mononucleosis without complication                                               :  0  \n Strain of other flexor muscle, fascia and tendon at forearm level, right arm, subsequent encounter:  0  \n NA's                                                                                              :734  \n                                                                                                         \n Unique.Visit       ActivityLevel    ActivityLevelF SwollenLymphNodes\n Length:735         Min.   : 0.000   3      :125    No :421          \n Class :character   1st Qu.: 3.000   5      : 97    Yes:314          \n Mode  :character   Median : 4.000   4      : 95                     \n                    Mean   : 4.463   2      : 80                     \n                    3rd Qu.: 6.000   7      : 68                     \n                    Max.   :10.000   6      : 66                     \n                                     (Other):204                     \n ChestCongestion ChillsSweats NasalCongestion CoughYN   Sneeze    Fatigue  \n No :326         No :131      No :170         No : 75   No :340   No : 64  \n Yes:409         Yes:604      Yes:565         Yes:660   Yes:395   Yes:671  \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n SubjectiveFever Headache      Weakness   WeaknessYN  CoughIntensity CoughYN2 \n No :230         No :115   None    : 49   No : 49    None    : 47    No : 47  \n Yes:505         Yes:620   Mild    :224   Yes:686    Mild    :156    Yes:688  \n                           Moderate:341              Moderate:360             \n                           Severe  :121              Severe  :172             \n                                                                              \n                                                                              \n                                                                              \n     Myalgia    MyalgiaYN RunnyNose AbPain    ChestPain Diarrhea  EyePn    \n None    : 79   No : 79   No :211   No :642   No :501   No :636   No :622  \n Mild    :214   Yes:656   Yes:524   Yes: 93   Yes:234   Yes: 99   Yes:113  \n Moderate:327                                                              \n Severe  :115                                                              \n                                                                           \n                                                                           \n                                                                           \n Insomnia  ItchyEye  Nausea    EarPn     Hearing   Pharyngitis Breathless\n No :316   No :553   No :477   No :573   No :705   No :121     No :438   \n Yes:419   Yes:182   Yes:258   Yes:162   Yes: 30   Yes:614     Yes:297   \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n ToothPn   Vision    Vomit     Wheeze       BodyTemp     \n No :569   No :716   No :656   No :514   Min.   : 97.20  \n Yes:166   Yes: 19   Yes: 79   Yes:221   1st Qu.: 98.20  \n                                         Median : 98.50  \n                                         Mean   : 98.94  \n                                         3rd Qu.: 99.30  \n                                         Max.   :103.10  \n                                         NA's   :5       \n                                RapidFluA  \n Positive for Influenza A            :169  \n Presumptive Negative For Influenza A:159  \n NA's                                :407  \n                                           \n                                           \n                                           \n                                           \n                                RapidFluB                        PCRFluA   \n Positive for Influenza B            : 26    Influenza A Detected    :120  \n Presumptive Negative For Influenza B:302    Influenza A Not Detected: 33  \n NA's                                :407   Assay Invalid            :  0  \n                                            Indeterminate            :  1  \n                                            NA's                     :581  \n                                                                           \n                                                                           \n                      PCRFluB     TransScore1    TransScore1F  TransScore2   \n  Influenza B Detected    :  9   Min.   :0.000   0: 13        Min.   :0.000  \n  Influenza B Not Detected:145   1st Qu.:3.000   1: 53        1st Qu.:2.000  \n Assay Invalid            :  0   Median :4.000   2:107        Median :3.000  \n NA's                     :581   Mean   :3.473   3:157        Mean   :2.917  \n                                 3rd Qu.:5.000   4:210        3rd Qu.:4.000  \n                                 Max.   :5.000   5:195        Max.   :4.000  \n                                                                             \n TransScore2F  TransScore3    TransScore3F  TransScore4    TransScore4F\n 0: 13        Min.   :0.000   0: 24        Min.   :0.000   0: 50       \n 1: 89        1st Qu.:1.000   1:166        1st Qu.:2.000   1:103       \n 2:138        Median :2.000   2:222        Median :3.000   2:154       \n 3:201        Mean   :2.148   3:323        Mean   :2.576   3:230       \n 4:294        3rd Qu.:3.000                3rd Qu.:4.000   4:198       \n              Max.   :3.000                Max.   :4.000               \n                                                                       \n  ImpactScore      ImpactScore2     ImpactScore3    ImpactScoreF ImpactScore2F\n Min.   : 2.000   Min.   : 2.000   Min.   : 0.00   8      :105   7      :107  \n 1st Qu.: 8.000   1st Qu.: 7.000   1st Qu.: 3.00   9      :104   8      :102  \n Median : 9.000   Median : 8.000   Median : 5.00   10     : 88   9      : 90  \n Mean   : 9.514   Mean   : 8.581   Mean   : 5.06   7      : 84   10     : 86  \n 3rd Qu.:11.000   3rd Qu.:10.000   3rd Qu.: 7.00   11     : 82   6      : 85  \n Max.   :18.000   Max.   :17.000   Max.   :13.00   12     : 58   11     : 59  \n                                                   (Other):214   (Other):206  \n ImpactScore3F ImpactScoreFD   TotalSymp1     TotalSymp1F    TotalSymp2   \n 4      :134   8      :105   Min.   : 5.00   12     : 86   Min.   : 4.00  \n 5      :112   9      :104   1st Qu.:11.00   13     : 84   1st Qu.:10.00  \n 3      :108   10     : 88   Median :13.00   14     : 80   Median :12.00  \n 6      :102   7      : 84   Mean   :12.99   11     : 72   Mean   :12.43  \n 7      : 66   11     : 82   3rd Qu.:15.00   10     : 62   3rd Qu.:15.00  \n 2      : 64   12     : 58   Max.   :23.00   15     : 61   Max.   :22.00  \n (Other):149   (Other):214                   (Other):290                  \n   TotalSymp3   \n Min.   : 3.00  \n 1st Qu.:10.00  \n Median :12.00  \n Mean   :11.66  \n 3rd Qu.:14.00  \n Max.   :21.00  \n                \n\n\nThe symptom variables are mostly categorical, and there are diagnostic and other coding variables in the “DxName”, “Impact”, and “TotalSymp” columns that we will remove to focus specifically on the predictor and outcome variables of interest.\nWe will go ahead and remove the extra columns, producing a data frame with 735 rows and 32 columns.\n\n#remove extra columns\ndata2 <- data %>% \n  select(!contains(\"Total\") &\n           !contains(\"Score\") &\n           !contains(\"FluA\") &\n           !contains(\"FluB\") &\n           !contains(\"Dxname\") &\n           !contains(\"Activity\")) %>% \n  select(!\"Unique.Visit\")\ndim(data2)\n\n[1] 735  32\n\n\nWe can also see that some variables have both a Yes/No question and a level of severity. We can remove the Yes/No columns and keep only the severity, which should remove 4 variables.\n\n#remove Yes/No columns if variable also has severity levels\ndata2 <- data2 %>% \n  select(!contains(\"WeaknessYN\") &\n           !contains(\"CoughYN\") &\n           !contains(\"CoughYN2\") &\n           !contains(\"MyalgiaYN\"))\ndim(data2)\n\n[1] 735  28\n\n\nNow that we have the columns we need, we will check for any missing data and remove the NAs.\n\n#check for NAs and drop\ngg_miss_var(data2)\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\nℹ The deprecated feature was likely used in the naniar package.\n  Please report the issue at <https://github.com/njtierney/naniar/issues>.\n\n\n\n\ncleandata <- drop_na(data2)"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#save-clean-data",
    "href": "fluanalysis/code/wrangling.html#save-clean-data",
    "title": "Wrangling",
    "section": "Save clean data",
    "text": "Save clean data\nThe data has been cleaned! We will save the cleaned data as cleandata in the data folder.\n\n#save cleaned data\nfilelocation <- here(\"fluanalysis\", \"data\", \"cleandata.rds\")\nsave(cleandata, file = filelocation)"
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "For this exercise, I will be using data cleaning and exploratory analysis on this week’s TidyTuesday data set. The data is on Hollywood Age Gaps showing the age difference between love interests on various films. The data set includes variables for Movie Title, Release Year, Director, and various character-related information."
  },
  {
    "objectID": "tidytuesday_exercise.html#possible-questions-to-explore",
    "href": "tidytuesday_exercise.html#possible-questions-to-explore",
    "title": "Tidy Tuesday Exercise",
    "section": "Possible questions to explore",
    "text": "Possible questions to explore\n\nDoes the average age difference change over different release years?\nDoes age difference differ between mixed-gender and same-gender couples?\nWhat directors have the highest average age difference?\n\nTo begin, let’s load the packages and the data set."
  },
  {
    "objectID": "tidytuesday_exercise.html#load-packages",
    "href": "tidytuesday_exercise.html#load-packages",
    "title": "Tidy Tuesday Exercise",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(naniar)\nlibrary(forcats)"
  },
  {
    "objectID": "tidytuesday_exercise.html#load-data-from-tidytuesday",
    "href": "tidytuesday_exercise.html#load-data-from-tidytuesday",
    "title": "Tidy Tuesday Exercise",
    "section": "Load data from TidyTuesday",
    "text": "Load data from TidyTuesday\nThis part of the code used taken from the readme file on the GitHub repository.\n\ntuesdata <- tidytuesdayR::tt_load('2023-02-14')\ntuesdata <- tidytuesdayR::tt_load(2023, week = 7)\n# load data and assign to age_gaps\nage_gaps <- tuesdata$age_gaps"
  },
  {
    "objectID": "tidytuesday_exercise.html#overview-of-data",
    "href": "tidytuesday_exercise.html#overview-of-data",
    "title": "Tidy Tuesday Exercise",
    "section": "Overview of data",
    "text": "Overview of data\nFirst, let’s see an overview of the data.\n\nglimpse(age_gaps)\n\nRows: 1,155\nColumns: 13\n$ movie_name         <chr> \"Harold and Maude\", \"Venus\", \"The Quiet American\", …\n$ release_year       <dbl> 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 199…\n$ director           <chr> \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joe…\n$ age_difference     <dbl> 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35,…\n$ couple_number      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ actor_1_name       <chr> \"Ruth Gordon\", \"Peter O'Toole\", \"Michael Caine\", \"D…\n$ actor_2_name       <chr> \"Bud Cort\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", \"T…\n$ character_1_gender <chr> \"woman\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", …\n$ character_2_gender <chr> \"man\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", \"…\n$ actor_1_birthdate  <date> 1896-10-30, 1932-08-02, 1933-03-14, 1930-09-17, 19…\n$ actor_2_birthdate  <date> 1948-03-29, 1982-06-03, 1982-10-01, 1975-11-08, 19…\n$ actor_1_age        <dbl> 75, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65,…\n$ actor_2_age        <dbl> 23, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30,…\n\nsummary(age_gaps)\n\n  movie_name         release_year    director         age_difference \n Length:1155        Min.   :1935   Length:1155        Min.   : 0.00  \n Class :character   1st Qu.:1997   Class :character   1st Qu.: 4.00  \n Mode  :character   Median :2004   Mode  :character   Median : 8.00  \n                    Mean   :2001                      Mean   :10.42  \n                    3rd Qu.:2012                      3rd Qu.:15.00  \n                    Max.   :2022                      Max.   :52.00  \n couple_number   actor_1_name       actor_2_name       character_1_gender\n Min.   :1.000   Length:1155        Length:1155        Length:1155       \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.398                                                           \n 3rd Qu.:2.000                                                           \n Max.   :7.000                                                           \n character_2_gender actor_1_birthdate    actor_2_birthdate     actor_1_age   \n Length:1155        Min.   :1889-04-16   Min.   :1906-10-06   Min.   :18.00  \n Class :character   1st Qu.:1953-05-16   1st Qu.:1965-03-25   1st Qu.:33.00  \n Mode  :character   Median :1964-10-03   Median :1974-07-30   Median :39.00  \n                    Mean   :1960-09-07   Mean   :1971-01-29   Mean   :40.64  \n                    3rd Qu.:1973-08-07   3rd Qu.:1982-04-07   3rd Qu.:47.00  \n                    Max.   :1996-06-01   Max.   :1996-11-11   Max.   :81.00  \n  actor_2_age   \n Min.   :17.00  \n 1st Qu.:25.00  \n Median :29.00  \n Mean   :30.21  \n 3rd Qu.:34.00  \n Max.   :68.00  \n\n\nThere are 1155 observations with 13 variables. The earliest movie is from 1935 and the most recent from 2022. The maximum age difference is 52 years, and the other variables can use age and birthday of the actors to calculate age difference. Next, we can check for missing data points.\n\n#check for missing points\ngg_miss_var(age_gaps)\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\nℹ The deprecated feature was likely used in the naniar package.\n  Please report the issue at <https://github.com/njtierney/naniar/issues>.\n\n\n\n\n#check for NAs\nany(is_na(age_gaps))\n\n[1] FALSE\n\n\nIt looks like this is a complete dataset! Next we can explore the data by each variable."
  },
  {
    "objectID": "tidytuesday_exercise.html#exploring-the-data",
    "href": "tidytuesday_exercise.html#exploring-the-data",
    "title": "Tidy Tuesday Exercise",
    "section": "Exploring the data",
    "text": "Exploring the data\nSince age_difference is our outcome of interest, we can look at the distribution of age differences.\n\n#histogram of age differences\nhist(age_gaps$age_difference,\n     labels = TRUE,\n     xlab = \"Difference in age by year\",\n     main = \"Histogram of Age Differences\",\n     ylim = c(0,450),\n     xlim = c(0,60))\n\n\n\n\nA majority of the age differences are below 20 years with most being below 5 years. The highest age difference is approximately 55 years.\nWe can also look at the distribution of observations based on release year.\n\n#table function for count of observations for each year\ntable(age_gaps$release_year)\n\n\n1935 1936 1937 1939 1940 1942 1944 1946 1947 1948 1950 1951 1952 1953 1954 1955 \n   2    1    3    1    3    2    1    1    1    4    1    3    4    3    7    2 \n1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 \n   3    4    5    2    2    4    3    5    4    5    1    7    1    3    2    3 \n1972 1973 1974 1975 1976 1977 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 \n   3    5    6    1    1    4    5    1    5    6    6    1    5    5    7    5 \n1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 \n   9    6    7   13   15   14   36   18   35   28   50   34   38   50   38   38 \n2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 \n  34   40   46   47   44   28   34   34   30   33   40   44   35   40   20    5 \n2021 2022 \n   8    5 \n\n\nMost of the data points are from ~1980 to 2019.\n\nQuestion 1\nNext, we can look age difference based on release year.\n\nggplot(age_gaps, aes(release_year, age_difference))+\n  geom_point()+\n  labs(x = \"Release Year\",\n       y = \"Age Difference\",\n       title = \"Age difference by release year\")\n\n\n\n\nNo clear relationship between release year and age difference but that could be due to a majority of the data points being from the last 50 years. We can also look at the average age difference per year. We can also look at the gender of the older character since character 1 is listed as the older character in the data set.\n\nggplot(age_gaps, aes(release_year, age_difference, color = character_1_gender))+\n  geom_point()+\n  labs(x = \"Release Year\",\n       y = \"Age Difference\",\n       title = \"Age difference by release year\",\n       subtitle = \"based on older character gender\")+\n  theme_minimal()+\n  scale_color_brewer(palette = \"Paired\")\n\n\n\n\nWhen the gender of the older character is added, there appears to be more older male characters compared to older female characters, and when females are older, the age difference tends to be below 15 years. There is an easily identifiable outlier around 1973 that has an age difference over 50.\nWe can also look at the mean age difference by release year.\n\nage_gaps %>% \n  group_by(release_year) %>% \n  summarise(mean_age_diff = mean(age_difference)) %>% #average age difference for each release year\n  ggplot(aes(release_year, mean_age_diff))+\n  geom_line(color = \"darkslateblue\")+\n  labs(title = \"Average age difference by release year\",\n       x = \"Release Year\",\n       y = \"Average Age Difference\")\n\n\n\n\nThere still doesn’t seem to be a clear relationship between release year and age difference, but there does seem to be a stabilization at approximately 10 years by 2000.\n\n\nQuestion 2\nI would also like to explore if there is a a difference between mixed-gender and same-gender couples. First, I’ll need to create a new variable called relationship to denote the groups.\n\n#create relationship variable and reassign to age_gaps\nage_gaps <- age_gaps %>% \n  mutate(relationship = case_when(character_1_gender == \"man\" & character_2_gender == \"woman\" ~ \"man_woman\",\n                                  character_1_gender == \"woman\" & character_2_gender == \"man\" ~ \"man_woman\", #needed to account for either gender as character 1\n                                  character_1_gender == \"man\" & character_2_gender == \"man\" ~ \"man_man\",\n                                  character_1_gender == \"woman\" & character_2_gender == \"woman\" ~ \"woman_woman\"))\n\nglimpse(age_gaps)\n\nRows: 1,155\nColumns: 14\n$ movie_name         <chr> \"Harold and Maude\", \"Venus\", \"The Quiet American\", …\n$ release_year       <dbl> 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 199…\n$ director           <chr> \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joe…\n$ age_difference     <dbl> 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35,…\n$ couple_number      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ actor_1_name       <chr> \"Ruth Gordon\", \"Peter O'Toole\", \"Michael Caine\", \"D…\n$ actor_2_name       <chr> \"Bud Cort\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", \"T…\n$ character_1_gender <chr> \"woman\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", …\n$ character_2_gender <chr> \"man\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", \"…\n$ actor_1_birthdate  <date> 1896-10-30, 1932-08-02, 1933-03-14, 1930-09-17, 19…\n$ actor_2_birthdate  <date> 1948-03-29, 1982-06-03, 1982-10-01, 1975-11-08, 19…\n$ actor_1_age        <dbl> 75, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65,…\n$ actor_2_age        <dbl> 23, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30,…\n$ relationship       <chr> \"man_woman\", \"man_woman\", \"man_woman\", \"man_woman\",…\n\n\nNow we can group by relationship and plot age difference.\n\nage_gaps %>% \n  ggplot()+\n  geom_boxplot(aes(relationship, age_difference))+\n  labs(x = \"Relationship Type\",\n       y = \"Age Difference\",\n       title = \"Age difference by relationship type\")\n\n\n\n\nIt looks like the man_woman and woman_woman tend to have a similar median age difference with man_man having a higher median age difference. The man_woman category also appears to be responsible for most of the high age differences as shown with the outliers.\n\n\nQuestion 3\nFinally, let’s look into the third exploratory question about directors and age differences.\n\n#count the number of directors in the dataset\nlength(unique(age_gaps$director))\n\n[1] 510\n\n\nThere are 510 directors in the dataset. Let’s graph age difference by director.\n\nggplot(age_gaps, aes(x = director, y = age_difference))+\n  geom_boxplot()+\n  labs()+\n  coord_flip()\n\n\n\n\nAlright, so we obviously have too many data points to really see any trends. Let’s arrange the data be descending age difference and only look at the top 20 directors\n\nage_gaps2 <- age_gaps %>% \n  group_by(director) %>% #group by director\n  summarise(mean_age_diff = mean(age_difference)) %>% #create average age difference for each director\n  arrange(desc(mean_age_diff)) %>% #order by descending age difference\n  slice(1:20) #use only the top 20 rows\n\nggplot(age_gaps2, aes(x = fct_reorder(director, mean_age_diff), y = mean_age_diff))+ #needed to use fct_reorder to display ascending age difference rather than alphabetical \n  geom_point(color = \"blue\")+\n  labs(y = \"Average Age Difference\", \n       x = \"Director\",\n       title = \"Directors with highest average age difference\")+\n  coord_flip()+ #flipped axes so director names were easier to read\n  scale_y_reverse() #flipped axis to display in descending order\n\n\n\n\nHal Ashby have the highest average age difference between characters by far, followed by Katt Shea, Jon Amiel, and Irving Pichel which all had age differences over 35 years."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "The data I will use is from the FiveThirtyEight article “Some People Are Too Superstitious To Have A Baby On Friday The 13th” and the associated GitHub repository. This article uses data from the National Center for Health Statistics (1994 - 2003) and the Social Security Administration (2000 - 2014). The two data files contain information on the year, month, date (as in number of the month), day of the week (1 represents Monday and 7 represents Sunday), and number of births for each date. For this exercise, I will be attempting to remake the first figure of the article shown below. \n\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggthemes)"
  },
  {
    "objectID": "visualization_exercise.html#data-loading",
    "href": "visualization_exercise.html#data-loading",
    "title": "Visualization Exercise",
    "section": "Data loading",
    "text": "Data loading\nI downloaded the data files and placed them into the data folder as births1 and births2. Using the read_delim() function, these data sets will be assigned as data1 and data2.\n\ndata1 <- read_delim(file = \"data/births1.txt\")\ndata2 <- read_delim(file = \"data/births2.txt\")\n\n\n#summary of datasets\nsummary(data1)\n\n      year          month        date_of_month    day_of_week     births     \n Min.   :1994   Min.   : 1.000   Min.   : 1.00   Min.   :1    Min.   : 6443  \n 1st Qu.:1996   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2    1st Qu.: 8844  \n Median :1998   Median : 7.000   Median :16.00   Median :4    Median :11615  \n Mean   :1998   Mean   : 6.524   Mean   :15.73   Mean   :4    Mean   :10877  \n 3rd Qu.:2001   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:6    3rd Qu.:12274  \n Max.   :2003   Max.   :12.000   Max.   :31.00   Max.   :7    Max.   :14540  \n\nsummary(data2)\n\n      year          month        date_of_month    day_of_week     births     \n Min.   :2000   Min.   : 1.000   Min.   : 1.00   Min.   :1    Min.   : 5728  \n 1st Qu.:2003   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2    1st Qu.: 8740  \n Median :2007   Median : 7.000   Median :16.00   Median :4    Median :12343  \n Mean   :2007   Mean   : 6.523   Mean   :15.73   Mean   :4    Mean   :11350  \n 3rd Qu.:2011   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:6    3rd Qu.:13082  \n Max.   :2014   Max.   :12.000   Max.   :31.00   Max.   :7    Max.   :16081  \n\n#check number of rows to identify potential issues when joining\nnrow(data1)\n\n[1] 3652\n\nnrow(data2)\n\n[1] 5479\n\n\nLooking at the summary of the datasets, data1 contains information for years 1994-2003, and data2 for years 2000-2014. The columns for month, date_of_month, and day_of_week match, and the second dataset has more observations compared to the first."
  },
  {
    "objectID": "visualization_exercise.html#join-data-sets-from-sources",
    "href": "visualization_exercise.html#join-data-sets-from-sources",
    "title": "Visualization Exercise",
    "section": "Join data sets from sources",
    "text": "Join data sets from sources\nThe graph uses data from the years 1994-2014, so we need to join the datasets together to analyze. According to the footnote, the data of the overlapping years did not greatly differ. We can now join the two data sets with full_join(). We should use full join rather than other _join() options in order to preserve as much data as possible.\n\n#join both data sets and assign to fulldata\nfulldata <- full_join(data1, data2)\n\nJoining, by = c(\"year\", \"month\", \"date_of_month\", \"day_of_week\", \"births\")\n\n#visual display of 10 random observations in fulldata set\nslice_sample(fulldata, n = 10)\n\n# A tibble: 10 × 5\n    year month date_of_month day_of_week births\n   <dbl> <dbl>         <dbl>       <dbl>  <dbl>\n 1  1995     4            15           6   8492\n 2  2003     3            13           4  12333\n 3  2011     9             1           4  13723\n 4  1997    10            21           2  11957\n 5  2010    11             8           1  12023\n 6  1998     2            10           2  12166\n 7  2001     4            11           3  12631\n 8  1996     7            19           5  12049\n 9  2009    11            11           3  12889\n10  2013    12             9           1  11824\n\n#summary of dataset and check for joining issues\nsummary(fulldata)\n\n      year          month        date_of_month    day_of_week     births     \n Min.   :1994   Min.   : 1.000   Min.   : 1.00   Min.   :1    Min.   : 5728  \n 1st Qu.:2000   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2    1st Qu.: 8787  \n Median :2003   Median : 7.000   Median :16.00   Median :4    Median :11998  \n Mean   :2004   Mean   : 6.523   Mean   :15.73   Mean   :4    Mean   :11161  \n 3rd Qu.:2008   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:6    3rd Qu.:12774  \n Max.   :2014   Max.   :12.000   Max.   :31.00   Max.   :7    Max.   :16081  \n\nstr(fulldata)\n\nspc_tbl_ [9,131 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ year         : num [1:9131] 1994 1994 1994 1994 1994 ...\n $ month        : num [1:9131] 1 1 1 1 1 1 1 1 1 1 ...\n $ date_of_month: num [1:9131] 1 2 3 4 5 6 7 8 9 10 ...\n $ day_of_week  : num [1:9131] 6 7 1 2 3 4 5 6 7 1 ...\n $ births       : num [1:9131] 8096 7772 10142 11248 11053 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   year = col_double(),\n  ..   month = col_double(),\n  ..   date_of_month = col_double(),\n  ..   day_of_week = col_double(),\n  ..   births = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nThe fulldata set now contains information for 1994-2014 with 9131 observations. The columns are the same as the original two datasets."
  },
  {
    "objectID": "visualization_exercise.html#manipulate-data",
    "href": "visualization_exercise.html#manipulate-data",
    "title": "Visualization Exercise",
    "section": "Manipulate data",
    "text": "Manipulate data\nNow that we have the data for the full range of years, we need to manipulate the data to find the average number of births on the 6th or 20th of the month compared to the average number of of births on the 13th of the month. Since the graph is separated by weekday, we need to first group by weekday.\n\n#assign to plot1data and create 3 new variables\nplot1data <- fulldata %>% group_by(day_of_week) %>% \n  summarise(avg620 = mean(births[date_of_month == 6 | date_of_month == 20]), #average number of births on each weekday for the 6th or 20th of the month\n            avg13 = mean(births[date_of_month == 13])) %>% #the average number of births on each weekday for the 13th of the month\n  mutate(pctdiff = (avg13-avg620)/avg13 *100) #the percent difference for each weekday\n\n#check the new data set\nhead(plot1data, 7)\n\n# A tibble: 7 × 4\n  day_of_week avg620  avg13 pctdiff\n        <dbl>  <dbl>  <dbl>   <dbl>\n1           1 11645. 11394.  -2.21 \n2           2 12870. 12600.  -2.15 \n3           3 12774. 12394.  -3.07 \n4           4 12713. 12280.  -3.52 \n5           5 12559. 11737.  -7.01 \n6           6  8655.  8600.  -0.641\n7           7  7636.  7560.  -1.00 \n\n\nI want to also recode the day_of_week column into days rather than numbers for the graph labeling. To preserve the weekday order, I will need to change the variable into a factor using mutate() and factor() functions.\n\n#recode days of the week for plot labels and factor to keep days ordered for graph\nplot1data<- plot1data %>% \n  mutate(day_of_week = case_when(\n    day_of_week %in% 1 ~ \"MON\",\n    day_of_week %in% 2 ~ \"TUES\",\n    day_of_week %in% 3 ~ \"WED\",\n    day_of_week %in% 4 ~ \"THURS\",\n    day_of_week %in% 5 ~ \"FRI\",\n    day_of_week %in% 6 ~ \"SAT\",\n    day_of_week %in% 7 ~ \"SUN\")) %>% \n  mutate(day_of_week = factor(day_of_week,\n        levels = c(\"MON\", \"TUES\", \"WED\", \"THURS\", \"FRI\", \"SAT\", \"SUN\")))\n  \n\n#check new dataset\nhead(plot1data, 7)\n\n# A tibble: 7 × 4\n  day_of_week avg620  avg13 pctdiff\n  <fct>        <dbl>  <dbl>   <dbl>\n1 MON         11645. 11394.  -2.21 \n2 TUES        12870. 12600.  -2.15 \n3 WED         12774. 12394.  -3.07 \n4 THURS       12713. 12280.  -3.52 \n5 FRI         12559. 11737.  -7.01 \n6 SAT          8655.  8600.  -0.641\n7 SUN          7636.  7560.  -1.00 \n\n\nThe new dataframe plot1data has a summary of the average number of births on the 6th or 20th and the average number of births on the 13th as well as the percent difference for each weekday. Notice that we needed to use the summarise() rather than the mutate() function to create the new variables. The summarise() function returns one row for a group of observations after a group_by() statement while mutate() returns the same number of rows as the original dataset."
  },
  {
    "objectID": "visualization_exercise.html#plot-graph",
    "href": "visualization_exercise.html#plot-graph",
    "title": "Visualization Exercise",
    "section": "Plot graph",
    "text": "Plot graph\nUsing the plot1data, we can attempt to recreate the graph using the ggplot() and associated functions. The geom_col() function will be the basis of the graph, and the theme(), labs(), scale_x_discrete(), scale_y_continuous() , and scale_fill_manual() functions all adjusted the graph elements to look like the original graph (or at least as close as I can get to it!).\nNote: Because I did not remove data for the national holidays that fall on either the 6th or 20th between 1994 to 2014, my graph data may not match the article’s exactly, but the trends are still present.\n\n#set up standard plot\nggplot(plot1data)+\n  geom_col(aes(x = day_of_week, y = pctdiff, fill = day_of_week), width = 0.75)+\n  theme_fivethirtyeight()+ #theme from article database\n\n#add labeling\n  labs(title = \"The Friday the 13th effect\",\n       subtitle = \"Difference in the share of U.S. births on the 13th of each month\\nfrom the average of births on the 6th and the 20th, 1994-2014\",\n       y = \"ppt\", x = NULL)+ # add title, subtitle, y axis title and remove x axis\n\n#add axis elements and bar colors\n  scale_x_discrete(position = \"top\")+ #move x axis to top of graph,\n  scale_y_continuous(breaks = c(0,-1,-2,-3,-4,-5,-6), labels = c(\"0 ppt\", \"-1\",\"-2\",\"-3\",\"-4\",\"-5\",\"-6\"))+ #define y axis tick marks and labels\n  scale_fill_manual(values =\n        c(\"#EBB7EC\", \"#EBB7EC\", \"#EBB7EC\", \"#EBB7EC\",\n                   \"#E22EDD\", \"#EBB7EC\", \"#EBB7EC\"))+ #bar colors\n                     \n#Adjust theme to match graph aesthetics\n  theme(panel.grid = element_line(colour = \"#E0E0E0\"), #light grey for grid lines\n        panel.background = element_rect(fill = \"#F0F0F0\"), #light grey for panel background\n        plot.background = element_rect(fill = \"#F0F0F0\" ), #light grey for graph background\n        axis.line.x = element_line(color = \"#808080\"), # black for x axis\n        axis.text = element_text(color = \"#b5b5b5\"), #grey color of axis labels\n        plot.title = element_text(face = \"bold\"), # bold title\n        plot.subtitle = element_text(size = 10), #change size of subtitle\n        legend.position = \"none\", # remove legend\n        axis.title.y = NULL) #remove y axis label\nggsave(\"remadegraph.png\", width = 5, height = 5)\n\n\nAfter adjusting the height and width of the final graph, the remade plot is pretty close to the original graph. The biggest differences that I could not code for is the font and removal of holidays."
  },
  {
    "objectID": "visualization_exercise.html#troublesissues",
    "href": "visualization_exercise.html#troublesissues",
    "title": "Visualization Exercise",
    "section": "Troubles/Issues",
    "text": "Troubles/Issues\nWhen I was trying to recreate the graph, there was a lot of googling to figure out all the theme elements to adjust (particularly with this website), but my main issue was trying to get the correct data for percent change needed for the y axis. Google did not have that answer I was looking for since many of the solutions involved something along the line of y = (..count..)/sum(..count..) in ggplot. Eventually, I figured out how to manually code it correctly after determining exactly what was being compared on the graph. My notes from the Introduction to R class that I took last semester were very helpful for manually coding plot1data."
  }
]