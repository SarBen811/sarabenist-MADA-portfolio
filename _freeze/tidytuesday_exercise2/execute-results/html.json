{
  "hash": "8ca8ea30df85e028509e0e3618daf807",
  "result": {
    "markdown": "---\ntitle: \"Tidy Tuesday Exercise 2\"\noutput: html_document\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-depth: 2\n---\n\n\n***This exercise is will use the TidyTuesday data for this week to perform cross-validation, model tuning and fitting, and model evaluation.***\n\nThis exercise will be another use of the `Tidy Tuesday` data. We will be looking at US Egg Production. Cage-free hens and eggs will be compared to other table eggs in the US, looking at number of eggs produced, number of hens, and the percentage of cage-free eggs and hens. More information about the dataset can be found [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-04-11/readme.md).\n\n## Load packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(skimr)\nlibrary(ggplot2)\nlibrary(bonsai)\n```\n:::\n\n\n## Load data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data and assign to eggprod and cagefreeperct\neggprod  <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')\ncagefreeperct <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/cage-free-percentages.csv')\n```\n:::\n\n\n## Explore data\n\nLet's take a look at the data using `skim()`\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(eggprod)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |        |\n|:------------------------|:-------|\n|Name                     |eggprod |\n|Number of rows           |220     |\n|Number of columns        |6       |\n|_______________________  |        |\n|Column type frequency:   |        |\n|character                |3       |\n|Date                     |1       |\n|numeric                  |2       |\n|________________________ |        |\n|Group variables          |None    |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|prod_type     |         0|             1|  10|  13|     0|        2|          0|\n|prod_process  |         0|             1|   3|  23|     0|        3|          0|\n|source        |         0|             1|  23|  23|     0|      108|          0|\n\n\n**Variable type: Date**\n\n|skim_variable  | n_missing| complete_rate|min        |max        |median     | n_unique|\n|:--------------|---------:|-------------:|:----------|:----------|:----------|--------:|\n|observed_month |         0|             1|2016-07-31 |2021-02-28 |2018-11-15 |       56|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|       mean|         sd|        p0|       p25|        p50|        p75|       p100|hist  |\n|:-------------|---------:|-------------:|----------:|----------:|---------:|---------:|----------:|----------:|----------:|:-----|\n|n_hens        |         0|             1|  110839873|  124121204|  13500000|  17284500|   59939500|  125539250|  341166000|▇▁▁▁▂ |\n|n_eggs        |         0|             1| 2606667580| 3082457619| 298074240| 423962023| 1154550000| 2963010996| 8601000000|▇▁▁▁▂ |\n:::\n:::\n\n\nThis data set has 6 variables: 3 character (product type, product process, and source), 1 date (month of observation as Y-M-D), and 2 numeric(number of hens and number of eggs). All columns are complete with no missing data. The observations are from 2016 to 2021. The mean number of hens in the US is 110,839,873 which produced 2,606,667,580 eggs on average. That is a lot of eggs to gather!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(cagefreeperct)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |cagefreeperct |\n|Number of rows           |96            |\n|Number of columns        |4             |\n|_______________________  |              |\n|Column type frequency:   |              |\n|character                |1             |\n|Date                     |1             |\n|numeric                  |2             |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|source        |         0|             1|   8|  35|     0|       31|          0|\n\n\n**Variable type: Date**\n\n|skim_variable  | n_missing| complete_rate|min        |max        |median     | n_unique|\n|:--------------|---------:|-------------:|:----------|:----------|:----------|--------:|\n|observed_month |         0|             1|2007-12-31 |2021-02-28 |2018-11-15 |       91|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|   sd|   p0|   p25|   p50|   p75|  p100|hist  |\n|:-------------|---------:|-------------:|-----:|----:|----:|-----:|-----:|-----:|-----:|:-----|\n|percent_hens  |         0|          1.00| 17.95| 6.58| 3.20| 13.46| 17.30| 23.46| 29.20|▂▅▇▆▆ |\n|percent_eggs  |        42|          0.56| 17.10| 4.29| 9.56| 14.52| 16.23| 19.46| 24.55|▆▇▇▆▇ |\n:::\n:::\n\nThis dataset has 4 columns: 1 character (source of observation), 1 date (month of observation), and 2 numeric(percent cage-free hens and percent cage-free eggs). The observations occur between 2007 and 2021. Both hens and eggs have an average percent cage-free of 17%. This would be appropriate since cage-free hens will produce cage-free eggs. The percentage of cage-free eggs is missing approximately half of the data.\n\nWe can also make a few simple graphs to view the data in a different way.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot number of eggs by date\neggplot <- ggplot(eggprod)+\n  geom_point(aes(observed_month, n_eggs))+\n  labs(title = \"Number of eggs by date\", x = \"Date\", y = \"Number of eggs\")\neggplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\nWe can see there seems to be a trend based on some subtype based on the four clear linear trends in the graph. Let's adjust the graph a little bit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neggplot2 <- ggplot(eggprod, aes(observed_month, log(n_eggs)))+ #log scale the eggs to better see the data\n  geom_point(aes(color = prod_process))+ #color the data by production process\n  labs(title = \"Number of eggs by date\", x = \"Date\", y = \"Log number of eggs\")\neggplot2\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nThere is a trend based on cage-free vs table eggs and organic vs non-organic. We can look at a similar plot using the number of hens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot number of hens by date\nhenplot <- ggplot(eggprod, aes(observed_month, log(n_hens)))+ #log scale of hens\n  geom_point(aes(color = prod_process))+ #color the data by production process\n  labs(title = \"Number of hens by date\", x = \"Date\", y = \"Log number of hens\")\nhenplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nThe same trends from the egg plot appear in the hen plot which would make sense as the same category of hens are producing the category of eggs. We can also look at the cage-free percentage by date.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncageplot <- ggplot(cagefreeperct)+\n  geom_point(aes(observed_month, percent_eggs), color = \"red\")+ #creates scatter plot of %eggs in red\n  geom_point(aes(observed_month, percent_hens), color = \"darkgreen\")+ #creates scatter plot of %hens in green\n  labs(title = \"Number of cage-free eggs and hens by date\", x= \"Date\", y= \"Percentage\")\ncageplot\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 42 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nHere, we can see the percentage of cage-free hens and eggs have increased since 2015. \n\n## Wrangle data\nNow that we have a general idea about the data, we can wrangle and clean the data. First, I would be interested in looking at the average eggs per hen between cage-free and non-cage-free hens. We will use the `mutate()` function to make a new variable.\n\n::: {.cell}\n\n```{.r .cell-code}\neggprod <- eggprod %>% mutate(egg_production = n_eggs/n_hens)\nsummary(eggprod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n observed_month        prod_type         prod_process           n_hens         \n Min.   :2016-07-31   Length:220         Length:220         Min.   : 13500000  \n 1st Qu.:2017-09-30   Class :character   Class :character   1st Qu.: 17284500  \n Median :2018-11-15   Mode  :character   Mode  :character   Median : 59939500  \n Mean   :2018-11-14                                         Mean   :110839873  \n 3rd Qu.:2019-12-31                                         3rd Qu.:125539250  \n Max.   :2021-02-28                                         Max.   :341166000  \n     n_eggs             source          egg_production \n Min.   :2.981e+08   Length:220         Min.   :17.03  \n 1st Qu.:4.240e+08   Class :character   1st Qu.:20.66  \n Median :1.155e+09   Mode  :character   Median :23.25  \n Mean   :2.607e+09                      Mean   :22.43  \n 3rd Qu.:2.963e+09                      3rd Qu.:24.03  \n Max.   :8.601e+09                      Max.   :25.56  \n```\n:::\n:::\n\nNow we have the average number of eggs per hen which varies between 17 to 25 eggs per month.\n\n## Research question\nAfter looking at the data, the main research questions I want to look at is:\n  1) Do the cage-free and non-cage-free hens produce the same number of eggs on average? Is the average number of eggs produced by a hen expected to change over time?\n\nThe main outcome will be `egg_production` and the main predictors will be `prod_type`, `prod_process`, and the `observed_month`. We will go ahead and remove the `source` column from the data set due to the issues the column causes in the models. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#remove source column\neggprod <- eggprod %>% select(-source)\n```\n:::\n\n\n## Split data\n\n::: {.cell}\n\n```{.r .cell-code}\n#establish reproducibility by setting the seed\nset.seed(424)\n\n#stratify data split\ndata_split <- initial_split(eggprod, prop = 3/4)\n\n#create two data sets with 3/4 of data in training set\ntrain_egg <- training(data_split)\ntest_egg <- testing(data_split)\n```\n:::\n\n\n## Null Model\nFirst, we need to determine the performance of the null model which does not have any predictor variables. This will simple predict the mean egg production using the outcome values. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set null model using null_model() function\nnullmod <- null_model() %>% \n  set_engine(\"parsnip\") %>% \n  set_mode(\"regression\")\n\n#create null recipe\nnull_rec <- recipe(egg_production ~ ., data = train_egg)%>% \n  step_date(observed_month) %>% #changes date column into nominal\n  step_rm(observed_month) %>% #removes original date column\n  step_dummy(all_nominal_predictors()) #creates dummy variables for predictors\n\n#create null workflow\nnullmodwf <- workflow() %>% \n  add_model(nullmod) %>% \n  add_recipe(null_rec)\n\n#fit the null model to training data\nnullmodfit <- nullmodwf %>% \n  fit(data = train_egg)\n```\n:::\n\n\nWe have fit the model to the training data. Since this is a regression (the outcome is continuous), the `rmse()` function will produce the metric for this model.\n\n::: {.cell}\n\n```{.r .cell-code}\n#use null model to make predictions from training data\nnullmod_predtrain <- augment(nullmodfit, train_egg) %>% \n  rmse(truth = egg_production, .pred) %>% \n  mutate(model = \"Null\")\nnullmod_predtrain\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model\n  <chr>   <chr>          <dbl> <chr>\n1 rmse    standard        2.08 Null \n```\n:::\n:::\n\nThis prediction model based on the training data has an RMSE of *2.078.* We can also make predictions using the testing data in order to see how the model performs on \"new\" data.\n\n::: {.cell}\n\n```{.r .cell-code}\n#use null model to make predictions from test data\nnullmod_predtest <-augment(nullmodfit, test_egg) %>% \n  rmse(egg_production, .pred) %>% \n  mutate(model = \"Null\")\nnullmod_predtest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model\n  <chr>   <chr>          <dbl> <chr>\n1 rmse    standard        2.49 Null \n```\n:::\n:::\n\nThe test null model performs slightly worse than the training data with a RMSE of *2.485*.\n\n## 4 models\nUsing the null model as the baseline, we can create 4 different models to compare the performance on the training data. To set this up, we can make the cross-validation folds and recipe that will be used in all four models. The four models we will explore are LASSO (linear regression), random forest model, decision tree model, and boosted tree model. \n\n::: {.cell}\n\n```{.r .cell-code}\n#set seed for reproducibility\nset.seed(424)\n#create cross-validation folds\nfolds <- vfold_cv(train_egg, v = 5)\n\n#create basic recipe that will be used in all models\n#step_ functions create dummy variables\nml_rec <- recipe(egg_production ~., data = train_egg) %>% \n  step_date(observed_month) %>% #changes date column into nominal\n  step_rm(observed_month) %>% #removes original date column\n  step_dummy(all_nominal_predictors()) #creates dummy variables for predictors\n```\n:::\n\n\n### Linear Regression (LASSO)\nStarting with the LASSO model, we will create our regression model with the identified tuning parameters and the model workflow.\n\n::: {.cell}\n\n```{.r .cell-code}\n#set LASSO regression model with tuning parameters based on engine specs\nlr_mod <- linear_reg(penalty = tune(), #penalty given for number of predictors\n                     mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n#create workflow using model and recipe\nlr_wf <- workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n\nThen set up a grid of the tuned parameters that will be used for cross-validation.\n\n::: {.cell}\n\n```{.r .cell-code}\n#set up tuning grid of parameters for cross validation\nlr_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30)) #creates a grid of penalty values to tune\n\n#tune parameters using cross-validation folds\nset.seed(424)\nlr_res <- lr_wf %>% \n  tune_grid(\n    resamples = folds,\n    grid = lr_grid,\n    control = control_grid(save_pred = TRUE) #saves the predictions for later\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'glmnet' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'Matrix' was built under R version 4.2.2\n```\n:::\n:::\n\nWe can plot the models using `autoplot()` which plots the tuning parameters by RMSE value.\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_res %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\nThe goal is to have a low RMSE value which not overfitting the model with too many predictors.\n\nNext, we will select the best model, set up the final workflow, and refit the model to the training data. From there, we will pull the RMSE of the fitted model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#select best LASSO model\nbest_lr <- lr_res %>% select_best(\"rmse\")\nbest_lr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0117 Preprocessor1_Model21\n```\n:::\n\n```{.r .cell-code}\n#summary of best model to be used for later comparison\ncompare_lr <- lr_res %>% \n  show_best(\"rmse\", n=1) %>% \n  select(c(.metric, mean, std_err)) %>% \n  mutate(model = \"Linear Regression (LASSO)\")\n\n#set up workflow\nlrfinal_wf <- lr_wf %>% \n  finalize_workflow(best_lr)\n\n#fit the final model\nlrfinal_fit <- lrfinal_wf %>% \n  fit(train_egg) \n\n#extract RMSE value\nlrfinalfitted <- augment(lrfinal_fit, train_egg)\n\nlrfinalfitted_rmse <- lrfinalfitted %>%\n  select(egg_production, .pred) %>% \n  rmse(truth = egg_production, .pred) %>% \n  mutate(model = \"Linear Regression (LASSO)\")\nlrfinalfitted_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model                    \n  <chr>   <chr>          <dbl> <chr>                    \n1 rmse    standard       0.294 Linear Regression (LASSO)\n```\n:::\n:::\n\nThe best fitted LASSO model has a penalty value of 0.0117 and an RMSE of *0.294*.\n\nIn addition to the RMSE value, we can also visualize model performance by looking at the residual plot. The residual plot should not have any identifiable patterns.\n\n::: {.cell}\n\n```{.r .cell-code}\n#creates residual variable from outcome and prediction values\nlrfinalfitted <- lrfinalfitted %>% \n  mutate(.resid = egg_production - .pred)\n\n#create residual plot\nlr_residplot <- lrfinalfitted %>% ggplot(aes(.pred, .resid))+\n  geom_point()+\n  geom_hline(yintercept = 0)+\n  labs(title = \"Linear Regression Model\")\nlr_residplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\nThe residual plot shows two clear groupings around predictions of ~19 and ~23.5 eggs per hen. \n\n### Random Forest model\nThe next model we will test is the random forest model. Start with setting up the model and workflow.\n\n::: {.cell}\n\n```{.r .cell-code}\n#detect your computers cores since rf models use parallel processing \ncores <- parallel::detectCores()\n\n#set random forest model\nrf_mod <- rand_forest(mtry = tune(), #parameter to tune based on engine\n                      min_n = tune(), \n                      trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% #number of threads for processing\n  set_mode(\"regression\")\n\n#set workflow\nrf_wf <- workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n\nThen we can tune the parameters of the model. The random forest does not use a separate `grid_regular` function.\n\n::: {.cell}\n\n```{.r .cell-code}\n#tune parameters using cross-validation folds\nset.seed(424)\nrf_res <- rf_wf %>% \n  tune_grid(folds,\n            grid = 25, #25 models within the grid\n            control = control_grid(save_pred = TRUE), #save model predictions\n            metrics = metric_set(rmse)) #establishes metric of model\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ranger' was built under R version 4.2.2\n```\n:::\n:::\n\nAutoplotting shows the RMSE of the models based on various tuning parameter estimates.\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\nThen we can select the best model, finalize the workflow, and fit the model to the training data. The RMSE from the fitted model is displayed as well.\n\n::: {.cell}\n\n```{.r .cell-code}\n#select best random forest model\nbest_rf <- rf_res %>% select_best(\"rmse\")\nbest_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1    16     3 Preprocessor1_Model09\n```\n:::\n\n```{.r .cell-code}\n#summary of best model to be used for later comparison\ncompare_rf <- rf_res %>% \n  show_best(\"rmse\", n=1) %>% \n  select(c(.metric, mean, std_err)) %>% \n  mutate(model = \"Random Forest\")\n\n#set up workflow\nrffinal_wf <- rf_wf %>% \n  finalize_workflow(best_rf)\n\n#fit the final model\nrffinal_fit <- rffinal_wf %>% \n  fit(train_egg) \n\n#extract RMSE value\nrffinalfitted<- augment(rffinal_fit, train_egg) \n\nrffinalfitted_rmse <- rffinalfitted%>% \n  select(egg_production, .pred) %>% \n  rmse(truth = egg_production, .pred)%>% \n  mutate(model = \"Random Forest\")\nrffinalfitted_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model        \n  <chr>   <chr>          <dbl> <chr>        \n1 rmse    standard       0.134 Random Forest\n```\n:::\n:::\n\nThe best random forest model has a mtry of 16, a minimum node size of 3, and a RMSE of *0.138*. \n\nWe can once again look at the residual plot to better observe model performance.\n\n::: {.cell}\n\n```{.r .cell-code}\n#create\nrffinalfitted <- rffinalfitted %>% \n  mutate(.resid = egg_production - .pred)\n\nrf_residplot <- rffinalfitted %>% ggplot(aes(.pred, .resid))+\n  geom_point()+\n  geom_hline(yintercept = 0)+\n  labs(title = \"Random Forest Model\")\nrf_residplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\nThis model also appears to have two clear groupings of data, but these groupings are tighter than the linear regression model.\n\n### Decision tree model\nNext we will look at the decision tree model. This model partitions data using if/then decisions, so the predictions end up looking like average egg_production of a subset of data based on predictor variables.\n\nWe can start by setting up the model and workflow.\n\n::: {.cell}\n\n```{.r .cell-code}\n#set decision tree model regression\ndt_mod <- decision_tree(\n  min_n = tune(), #tuning parameter based on engine\n  tree_depth = tune(),\n  cost_complexity = tune()\n) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n\n#create machine learning workflow\ndt_wf <- workflow() %>% \n  add_model(dt_mod) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n\nThen set up the grid of tuning parameter values and use cross-validation folds to tune the parameters.\n\n::: {.cell}\n\n```{.r .cell-code}\n#set up tuning grid of parameters for cross validation\ndt_grid <- grid_regular(min_n(),\n                        tree_depth(),\n                        cost_complexity())\n\n#tune parameters using cross-validation folds\nset.seed(424)\ndt_res <- dt_wf %>% \n  tune_grid(\n    resamples = folds, #CV folds\n    grid = dt_grid,\n    control = control_grid(save_pred = TRUE) #saves predictions\n  )\n```\n:::\n\n\nAutoplotting the models will give an idea of the best performing models based on various values of the tuning parameters. \n\n::: {.cell}\n\n```{.r .cell-code}\ndt_res %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\nThen we can select the best model, fit the model to the training data, and finalize the workflow. The RMSE will be extracted from the fitted model.\n\n::: {.cell}\n\n```{.r .cell-code}\n#select best tree model\nbest_dt <- dt_res %>% select_best(\"rmse\")\nbest_dt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config              \n            <dbl>      <int> <int> <chr>                \n1      0.00000316         15     2 Preprocessor1_Model16\n```\n:::\n\n```{.r .cell-code}\n#summary of best model to be used for later comparison\ncompare_dt <- dt_res %>% \n  show_best(\"rmse\", n=1) %>% \n  select(c(.metric, mean, std_err)) %>% \n  mutate(model = \"Decision Tree\")\n\n#set up workflow\ndtfinal_wf <- dt_wf %>% \n  finalize_workflow(best_dt)\n\n#fit the final model\ndtfinal_fit <- dtfinal_wf %>% \n  fit(train_egg) \n\n#extract RMSE value\ndtfinalfitted <- augment(dtfinal_fit, train_egg)\n\ndtfinalfitted_rmse <- dtfinalfitted%>% \n  select(egg_production, .pred) %>% \n  rmse(truth = egg_production, .pred)%>% \n  mutate(model = \"Decision Tree\")\ndtfinalfitted_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model        \n  <chr>   <chr>          <dbl> <chr>        \n1 rmse    standard      0.0190 Decision Tree\n```\n:::\n:::\n\nThe best performing decision tree model has a cost complexity value of 3.162278e-06, tree_depth of 15, and a mininum node size of 2. The RMSE is *0.019*.\n\nThe residuals can also be plotted to visualize model performance. \n\n::: {.cell}\n\n```{.r .cell-code}\n#create residuals from predictions and outcome values\ndtfinalfitted <- dtfinalfitted %>% \n  mutate(.resid = egg_production - .pred)\n\n#plot residuals and predictions\ndt_residplot <- dtfinalfitted %>% ggplot(aes(.pred, .resid))+\n  geom_point()+\n  geom_hline(yintercept = 0)+\n  labs(title = \"Decision Tree Model\")\ndt_residplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\nThe residual plot has a similar pattern to the other models with two identifiable groupings; however, due to the nature of the model engine, the residuals tend to line up into smaller groups as well which is shown by the grouping on the horizontal zero line. \n\n### Boosted tree model\nFinally, we will look into a boosted tree model using the `lightgbm` engine. \n\n::: {.cell}\n\n```{.r .cell-code}\n#set boosted tree regression model\nbt_mod <- boost_tree(tree_depth = tune(), #tuning parameters from engine specs\n                    trees = tune(),\n                    min_n = tune()) %>%\n  set_engine(\"lightgbm\") %>%\n  set_mode(\"regression\")\n\n#create boosted tree workflow\nbt_wf <- workflow() %>% \n  add_model(bt_mod) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n\nThen we will set up the tuning grid and resample the training data using the cross-validation folds. The cross-validation resampling will take a while to run and produces 9 models for each fold (3 options for the 3 tuning parameters).\n\n::: {.cell}\n\n```{.r .cell-code}\n#set up tuning grid of parameters for cross validation\nbt_grid <- grid_regular(tree_depth(),\n                        trees(),\n                        min_n())\n\n#tune parameters using cross-validation folds\nset.seed(424)\nbt_res <- bt_wf %>% \n  tune_grid(\n    resamples = folds,\n    grid = bt_grid,\n    control = control_grid(save_pred = TRUE)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'lightgbm' was built under R version 4.2.3\n```\n:::\n:::\n\nAutoplotting the models visually shows the best models based on lowest RMSE.\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_res %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\nThen we can select the best model, finalize the workflow, and fit the best model to the training data. We can again extract the RMSE value from the fitted model.\n\n::: {.cell}\n\n```{.r .cell-code}\n#select best tree model\nbest_bt <- bt_res %>% select_best(\"rmse\")\nbest_bt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  trees min_n tree_depth .config              \n  <int> <int>      <int> <chr>                \n1  2000     2          1 Preprocessor1_Model03\n```\n:::\n\n```{.r .cell-code}\n#summary of best model to be used for later comparison\ncompare_bt <- bt_res %>% \n  show_best(\"rmse\", n=1) %>% \n  select(c(.metric, mean, std_err)) %>% \n  mutate(model = \"Boosted Tree\")\n\n#set up workflow\nbtfinal_wf <- bt_wf %>% \n  finalize_workflow(best_bt)\n\n#fit the final model\nbtfinal_fit <- btfinal_wf %>% \n  fit(train_egg) \n\n#extract RMSE value\nbtfinalfitted <- augment(btfinal_fit, train_egg)\n\nbtfinalfitted_rmse <- btfinalfitted %>% \n  select(egg_production, .pred) %>% \n  rmse(truth = egg_production, .pred)%>% \n  mutate(model = \"Boosted Tree\")\nbtfinalfitted_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model       \n  <chr>   <chr>          <dbl> <chr>       \n1 rmse    standard       0.139 Boosted Tree\n```\n:::\n:::\n\nThe best boosted tree model has 2000 trees, minimal node size of 2, and a tree depth of 1, which produces a RMSE of *0.104*.\n\nUsing the same method, plot the residuals from the model.\n\n::: {.cell}\n\n```{.r .cell-code}\n#create residuals from predictions and outcome values\nbtfinalfitted <- btfinalfitted %>% \n  mutate(.resid = egg_production - .pred)\n\n#plot residuals and predictions\nbt_residplot <- btfinalfitted %>% ggplot(aes(.pred, .resid))+\n  geom_point()+\n  geom_hline(yintercept = 0)+\n  labs(title = \"Boosted Tree Model\")\nbt_residplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\nSimilar to the other models, the residual model shows two clear groupings in the data. \n\n## Compare models and choose best\nNow that we have the residual plots and RMSE values for the model, we will compare and choose the best performing model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare <- bind_rows(compare_lr, compare_rf, compare_dt, compare_bt)\ncompare\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  .metric  mean std_err model                    \n  <chr>   <dbl>   <dbl> <chr>                    \n1 rmse    0.326  0.0181 Linear Regression (LASSO)\n2 rmse    0.365  0.0251 Random Forest            \n3 rmse    0.439  0.0262 Decision Tree            \n4 rmse    0.232  0.0121 Boosted Tree             \n```\n:::\n\n```{.r .cell-code}\ncomparefit <- bind_rows(nullmod_predtrain, lrfinalfitted_rmse, rffinalfitted_rmse, dtfinalfitted_rmse, btfinalfitted_rmse)\ncomparefit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 4\n  .metric .estimator .estimate model                    \n  <chr>   <chr>          <dbl> <chr>                    \n1 rmse    standard      2.08   Null                     \n2 rmse    standard      0.294  Linear Regression (LASSO)\n3 rmse    standard      0.134  Random Forest            \n4 rmse    standard      0.0190 Decision Tree            \n5 rmse    standard      0.139  Boosted Tree             \n```\n:::\n:::\n\nAll models performed better than the null model. Based on the RMSE values, the linear regression performed the worse of all models. The decision tree model, boosted tree model, and the random forest model have the smallest uncertainty. The boosted tree model has the lowest RMSE estimate. Next, we can look at the residual models to decide between these models.\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_residplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndt_residplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-37-2.png){width=672}\n:::\n\n```{.r .cell-code}\nbt_residplot\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-37-3.png){width=672}\n:::\n:::\n\nThe decision tree model has by far the lowest residuals as shown through the grouping on the horizontal zero line and low residual values. The residual plot and RMSE value identify the decision tree as the best model.\n\n## Final fit using the test data\n\nUsing the decision tree model, we can evaluate model performance one final time on the test data. This will act as \"new data\" to determine how the model would perform on untrained/unfitted data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit final model to test data\ndttest_fit <- dtfinal_wf %>% \n  last_fit(data_split) #uses best model on test data\n\n#collect RMSE from final model\ndttest_fit %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.430 Preprocessor1_Model1\n2 rsq     standard       0.975 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\nnullmod_predtest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model\n  <chr>   <chr>          <dbl> <chr>\n1 rmse    standard        2.49 Null \n```\n:::\n:::\n\nThe boosted tree model performed better than the null model on the test data. The model performed worse on the test data compared to the training data *(0.01900938 vs 0.4304961)*, but this is to be expected. In addition, we can plot the prediction and residual plots for the test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create predictions and residuals for test data\ndttest_resd <- augment(dttest_fit) %>% \n  select(c(.pred, egg_production)) %>% \n  mutate(.resid = egg_production - .pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plots LASSO predictions and body temperature for test data\ndttest_plotp <- ggplot(dttest_resd)+\n  geom_point(aes(egg_production, .pred))+\n  geom_abline(slope = 1, yintercept = 17)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in geom_abline(slope = 1, yintercept = 17): Ignoring unknown parameters:\n`yintercept`\n```\n:::\n\n```{.r .cell-code}\ndttest_plotp\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\nOnce again, we can see the two groupings in data. But, the goal of the prediction plot is to have the points lie on a diagonal line, so it looks good so far! Next, looking at the residual plot.\n\n::: {.cell}\n\n```{.r .cell-code}\n#plots residuals and egg production for test data\ndttest_plotr <- ggplot(dttest_resd)+\n  geom_point(aes(egg_production,.resid))+\n  geom_hline(yintercept = 0)\ndttest_plotr\n```\n\n::: {.cell-output-display}\n![](tidytuesday_exercise2_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\nThe residual plot appears more spread out compared to the test data, but most of the data is grouped around the horizontal zero line. The left grouping seems to consistently have negative residual values while the opposite is true for the higher values. \n\n## Discussion\nAs a summary, the exercise used cross-validation to tune 4 models attempting to prediction the outcome (`egg_production`) from three predictors (`prod_type`, `prod_process`, and `observed_month`). The four models (linear regression using LASSO, random forest, decision tree, and boosted tree model) all performed better than the null model (RMSE = 2.0786768) and had RMSE values ranging from 0.01900938 (Decision Tree) to 0.29370228 (Linear Regression). Due to the small residual values, low RMSE value, and no abnormal data patterns (compared to the other model plots), I choose the decision tree model for the final fit on the test data. The decision tree model on the test data had a RMSE of 0.4304961 and an R^2 of 0.97. These measure show a high performing model on the new data. However, both the RMSE value and residual plot show a worse performing model compared to using the training data. ",
    "supporting": [
      "tidytuesday_exercise2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}