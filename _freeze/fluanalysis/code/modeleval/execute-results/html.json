{
  "hash": "fe6f2f2da8693eb1511c97e6f54383cd",
  "result": {
    "markdown": "---\ntitle: \"Model Evaluation\"\noutput: html_document\n---\n\n\n***This exercise is the fourth in the flu analysis series. This page will use the `tidymodels` guide to split the data into testing and training sets and run regression and classification models on the two main outcomes of interest: the main categorical outcome `Nausea` and main continuous outcome `BodyTemp`.***\n\n## Load packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(here)\nlibrary(rsample)\n```\n:::\n\n\n## Load data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load data\nfilelocation <- here(\"fluanalysis\", \"data\", \"cleandata.rds\")\nload(filelocation)\n#reassign as flu\nflu <- cleandata\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nflu <- flu %>% \n  select(!c(\"Vision\",\"Hearing\")) #remove columns with <50 entries in one category\n```\n:::\n\n\n## Data splitting\n\nTo start model evaluation, we will first split the data into training and testing sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#establish reproducibility by setting the seed\nset.seed(123)\n\n#add data to the training set\ndata_split <- initial_split(flu, prop = 3/4)\n\n#create two data sets with 3/4 of data in training set\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n```\n:::\n\n\n\n## Create a recipe and workflow from all symptoms\n\nWe will first create a recipe for a logistic regression model predicting `nausea` from all predictor variables. The recipe uses the `recipe()` function and will contain the formula and the data (the training set).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflu_rec <- \n  recipe(Nausea ~ . , data = train_data) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n```\n:::\n\n\nNext we will set a model workflow to pair the model and recipe together. This will help when evaluating model based on the training and testing data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set model\nlr_mod <- logistic_reg() %>% \n  set_engine(\"glm\")\n\nflu_wflow <-\n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_rec)\n```\n:::\n\n\nWe can create one function that will create the recipe and train the model using the `workflow()` and `fit()` functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflu_fit <-\n  flu_wflow %>% \n  fit(data = train_data)\n```\n:::\n\n\nTo check the fitted model, the `extract_fit_parsnip()` function will display the fitted model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflu_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 × 5\n   term                  estimate std.error statistic p.value\n   <chr>                    <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)            -2.83      9.07      -0.312  0.755 \n 2 BodyTemp               -0.0101    0.0921    -0.109  0.913 \n 3 SwollenLymphNodes_Yes  -0.435     0.235     -1.85   0.0649\n 4 ChestCongestion_Yes     0.277     0.252      1.10   0.271 \n 5 ChillsSweats_Yes        0.333     0.355      0.939  0.348 \n 6 NasalCongestion_Yes     0.233     0.300      0.778  0.437 \n 7 Sneeze_Yes              0.115     0.253      0.455  0.649 \n 8 Fatigue_Yes             0.288     0.460      0.626  0.531 \n 9 SubjectiveFever_Yes     0.402     0.271      1.48   0.138 \n10 Headache_Yes            0.644     0.367      1.75   0.0794\n# … with 22 more rows\n```\n:::\n:::\n\n\n## Predict from trained model\n\nNext, we can use the `test_data` set to predict from the trained model by using the `flu_fit`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(flu_fit, test_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 Yes        \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 Yes        \n# … with 173 more rows\n```\n:::\n:::\n\n\nThis output is not terribly helpful. The predicted probability of having nausea can be found by using the `augment()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflu_aug <- augment(flu_fit, test_data)\n```\n:::\n\n\nTo evaluate the performance of the model, we will use the ROC curve and ROC-AUC as the metrics. Ideally, the model should have at least a value of 0.7 to be useful.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(flu$Nausea)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"No\"  \"Yes\"\n```\n:::\n\n```{.r .cell-code}\n#generate ROC curve\nflu_aug %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>% #must specify \"second\" since the positive event is \"Yes\" which is the second level\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#generate ROC-AUC\nflu_aug %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.709\n```\n:::\n:::\n\n\nThe area under the ROC curve is 0.706 which indicates the model is somewhat useful.\n\nWe can also use the `train_data` to predict the from the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predict from training data\nflu_aug2 <- augment(flu_fit, train_data)\n\n#generate ROC curve\nflu_aug2 %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#generate ROC-AUC\nflu_aug2 %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.798\n```\n:::\n:::\n\n\nThe ROC-AUC is higher with the `train_data` than the `test_data` at 0.80 which is understandable since the model was fitted to the `train_data` set.\n\n## Create recipe with runny nose as predictor\n\nUsing all the same steps as above, we can predict nausea from runny nose.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create recipe\nflu_recRN <- \n  recipe(Nausea ~ RunnyNose, data = train_data)%>%\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n\n#create work flow\nflu_wflowRN <-\n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(flu_recRN)\n\n#create fitted model\nflu_fitRN <-\n  flu_wflowRN %>% \n  fit(data = train_data)\n```\n:::\n\n\nPredicting outcome from the `RunnyNose` fitted model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create predictions\nflu_augRN <- augment(flu_fitRN, test_data)\n\n#generate ROC curve\nflu_augRN %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#generate ROC-AUC\nflu_augRN %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.460\n```\n:::\n:::\n\n\nUsing the `test_data` and `RunnyNose` as the predictor, the ROC-AUC is 0.460, indicating that the model is not helpful in predicting nausea.\n\nEvaluate the fitted model using the `train_data`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predict from training data\nflu_augRN2 <- augment(flu_fit, train_data)\n\n#generate ROC curve\nflu_augRN2 %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#generate ROC-AUC\nflu_augRN2 %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.798\n```\n:::\n:::\n\n\nUsing the `train_data`, the ROC-AUC is 0.801 which is much different from the `test_data` evaluation. This is possibly due to the random distribution of the data, with the `train_data` having a higher correlation between `RunnyNose` and `Nausea` observations. This is a good example of why fitted models should not be evaluated using only the data used to fit the model.\n\n# This section added by Leah Lariscy\n\n## Create workflow for a linear regression\n\nThis will be used to predict body temp from all other variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_mod <- linear_reg() #define model\n\nrecipe_bodytemp <- recipe(BodyTemp ~ ., data = train_data)%>% #set recipe to predict body temp using all variables\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n\nbodytemp_lm_workflow <- workflow() %>% #combine model and recipe to make workflow\n  add_model(lm_mod) %>% \n  add_recipe(recipe_bodytemp)\n```\n:::\n\n\n## Data fitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(626)\nbodytemp_fit <- bodytemp_lm_workflow %>% \n  fit(data = train_data)\ntidy(bodytemp_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 × 5\n   term                  estimate std.error statistic   p.value\n   <chr>                    <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)            98.2        0.352   279.    0        \n 2 SwollenLymphNodes_Yes  -0.104      0.108    -0.960 0.337    \n 3 ChestCongestion_Yes     0.0566     0.114     0.496 0.620    \n 4 ChillsSweats_Yes        0.279      0.151     1.84  0.0664   \n 5 NasalCongestion_Yes    -0.213      0.133    -1.60  0.110    \n 6 Sneeze_Yes             -0.400      0.116    -3.44  0.000627 \n 7 Fatigue_Yes             0.375      0.187     2.00  0.0461   \n 8 SubjectiveFever_Yes     0.517      0.122     4.25  0.0000259\n 9 Headache_Yes           -0.0614     0.151    -0.406 0.685    \n10 Weakness_Mild          -0.0473     0.226    -0.209 0.834    \n# … with 22 more rows\n```\n:::\n:::\n\n\n## Model evaluation on training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbodytemp_aug <- augment(bodytemp_fit, train_data)\nbodytemp_aug %>% select(BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 547 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.5  99.4\n 2     98.6  98.5\n 3     98.5  99.0\n 4    101.   99.5\n 5     98.5  97.8\n 6    100.   99.3\n 7     98.3  98.6\n 8     98.7  98.6\n 9     98.5  99.2\n10    102.   99.2\n# … with 537 more rows\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug %>% \n  rmse(truth = BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nLooking at the plot, there doesn't appear to be a strong relationship here.\n\n## Model evaluation on testing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbodytemp_aug_2 <- augment(bodytemp_fit, test_data)\nbodytemp_aug_2 %>% select(BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.5\n 2    101.   98.9\n 3     98.8  99.0\n 4     98.5  98.7\n 5     98.1  98.5\n 6     98.4  99.0\n 7     99.5  99.5\n 8     98.8  99.7\n 9    102.   98.9\n10     99.7  99.6\n# … with 173 more rows\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug_2 %>% \n  rmse(truth = BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.14\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug_2 %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nNot seeing a strong relationship in the testing data either\n\n## Create workflow for linear regression pt. 2\n\nThis will be used to predict body temp from runny nose data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_mod <- linear_reg() #define model\n\nrecipe_bodytemp2 <- recipe(BodyTemp ~ RunnyNose, data = train_data) %>%  #set recipe to predict body temp using runny nose %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_ordinalscore()\n\nbodytemp_lm_workflow2 <- workflow() %>% #combine model and recipe to make workflow\n  add_model(lm_mod) %>% \n  add_recipe(recipe_bodytemp2)\n```\n:::\n\n\n## Data fitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(626)\nbodytemp_fit <- bodytemp_lm_workflow %>% \n  fit(data = train_data)\ntidy(bodytemp_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 × 5\n   term                  estimate std.error statistic   p.value\n   <chr>                    <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)            98.2        0.352   279.    0        \n 2 SwollenLymphNodes_Yes  -0.104      0.108    -0.960 0.337    \n 3 ChestCongestion_Yes     0.0566     0.114     0.496 0.620    \n 4 ChillsSweats_Yes        0.279      0.151     1.84  0.0664   \n 5 NasalCongestion_Yes    -0.213      0.133    -1.60  0.110    \n 6 Sneeze_Yes             -0.400      0.116    -3.44  0.000627 \n 7 Fatigue_Yes             0.375      0.187     2.00  0.0461   \n 8 SubjectiveFever_Yes     0.517      0.122     4.25  0.0000259\n 9 Headache_Yes           -0.0614     0.151    -0.406 0.685    \n10 Weakness_Mild          -0.0473     0.226    -0.209 0.834    \n# … with 22 more rows\n```\n:::\n:::\n\n\n## Model evaluation on training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbodytemp_aug3 <- augment(bodytemp_fit, train_data)\nbodytemp_aug3 %>% select(BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 547 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.5  99.4\n 2     98.6  98.5\n 3     98.5  99.0\n 4    101.   99.5\n 5     98.5  97.8\n 6    100.   99.3\n 7     98.3  98.6\n 8     98.7  98.6\n 9     98.5  99.2\n10    102.   99.2\n# … with 537 more rows\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug3 %>% \n  rmse(truth = BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug3 %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThere is a slight positive correlation but not really.\n\n## Model evaluation on testing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbodytemp_aug_4 <- augment(bodytemp_fit, test_data)\nbodytemp_aug_4 %>% select(BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.5\n 2    101.   98.9\n 3     98.8  99.0\n 4     98.5  98.7\n 5     98.1  98.5\n 6     98.4  99.0\n 7     99.5  99.5\n 8     98.8  99.7\n 9    102.   98.9\n10     99.7  99.6\n# … with 173 more rows\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug_4 %>% \n  rmse(truth = BodyTemp, .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.14\n```\n:::\n\n```{.r .cell-code}\nbodytemp_aug_4 %>% ggplot(aes(.pred, BodyTemp)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](modeleval_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nNo correlation\n\nOverall, I don't think the flu symptom is very predictive of body temperature.\n",
    "supporting": [
      "modeleval_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}