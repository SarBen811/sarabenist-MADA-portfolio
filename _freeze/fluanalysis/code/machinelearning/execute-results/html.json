{
  "hash": "bea0e38744ac18f39bea0c1bf7ffb5c4",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning\"\noutput: html_document\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-depth: 2\n---\n\n\n***This exercise is the fifth in flu analysis series. This page will follow the `tidymodels` method to train and fit three machine learning models on the main continuous outcome `BodyTemp`.***\n\nFor the machine learning exercise, we will use the `flu` data and focus on Body Temperature as the outcome. We will be using the Decision Tree model, LASSO model, and Random forest model to determine the best performing model.\n\n## Load packages\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(rsample)\nlibrary(rpart.plot)\nlibrary(vip)\nlibrary(glmnet)\nlibrary(ranger)\nlibrary(ggplot2)\nlibrary(ggfortify)\n```\n:::\n\n\n## Load data\n\n::: {.cell}\n\n```{.r .cell-code}\n#load data\nfilelocation <- here(\"fluanalysis\", \"data\", \"cleandata.rds\")\nload(filelocation)\n\n#reassign as flu\nflu <- cleandata\n\n#remove columns with <50 entries in one category\nflu <- flu %>% \n  select(!c(\"Vision\",\"Hearing\")) \n```\n:::\n\n\n## Data splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#establish reproducibility by setting the seed\nset.seed(123)\n\n#stratify data split to balance outcomes between data sets\ndata_split1 <- initial_split(flu, prop = 7/10, strata = BodyTemp)\n\n#create two data sets with 70% of data in training set\ntrain_data1 <- training(data_split1)\ntest_data1 <- testing(data_split1)\n```\n:::\n\n\n## Cross Validation & Workflow set up\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set seed for reproducibility\nset.seed(123)\nfolds <- vfold_cv(train_data1, v = 5)\n\n#create recipe for machine learning\n#step_ functions create dummy variables and order the severity columns\nml_rec <- recipe(BodyTemp ~., data = train_data1) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_ordinalscore()\n\n#set linear regression\nlm_mod <- linear_reg() \n\n#create machine learning workflow\nml_wf <- workflow() %>% \n  add_model(lm_mod) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n\n## Null model\nWe will use the null model as comparison for the other models. All other models should perform better than the null model since this model does not use any of the predictor variables.\n\n::: {.cell}\n\n```{.r .cell-code}\n#set null model using null_model() function\nnullmod <- null_model() %>% \n  set_engine(\"parsnip\") %>% \n  set_mode(\"regression\")\n\n#create null recipe\nnull_rec <- recipe(BodyTemp ~ ., data = train_data1)%>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_ordinalscore()\n\n#create null workflow\nnullmodwf <- workflow() %>% \n  add_model(nullmod) %>% \n  add_recipe(null_rec)\n\n#fit the null model to training data\nnullmodfit <- nullmodwf %>% \n  fit(data = train_data1)\n```\n:::\n\n\nThe null model can be used to make predictions of the outcome, but without any predictors, the model will produce the mean body temperature for the training and test data. The `rmse()` function will produce the metric for this model.\n\n::: {.cell}\n\n```{.r .cell-code}\n#use null model to make predictions from training data\nnullmod_predtrain <- augment(nullmodfit, train_data1) %>% \n  rmse(truth = BodyTemp, .pred)\nnullmod_predtrain\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.21\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#use null model to make predictions from test data\nnullmod_predtest <-augment(nullmodfit, test_data1) %>% \n  rmse(BodyTemp, .pred)\nnullmod_predtest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.16\n```\n:::\n:::\n\nThe training null model has a RMSE of *1.209* and the test null model performs slightly better with a RMSE of *1.163*.\n\n## Model tuning and fitting\nFor each model, we will use the `tune()` function to find the best parameters during the cross-validation step. After finding the best model based on our performance metric RMSE, we will fit the best model. \n\n### Tree model specification\n\n::: {.cell}\n\n```{.r .cell-code}\n#set up decision tree model and tune the parameters\ntune_spec <- decision_tree(\n  cost_complexity = tune(), #one parameter we will tune\n  tree_depth = tune() #another parameter\n) %>% \n  set_engine(\"rpart\") %>% #specify engine\n  set_mode(\"regression\") #regression due to continuous outcome\n```\n:::\n\n\n### Tree model workflow\n\n::: {.cell}\n\n```{.r .cell-code}\n#create decision tree workflow\ntree_wf <- workflow() %>% \n  add_model(tune_spec) %>% \n  add_recipe(ml_rec) #can use the general machine learning recipe\n```\n:::\n\n### Tree model grid specification\nThe `grid_regular()` will produce values of the parameters that we test to find the best model. For each value of tree depth, 5 cost_complexity values will be tested.\n\n::: {.cell}\n\n```{.r .cell-code}\n#create grid of resampled values for cross-validation\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5) #produces 5x5 = 25 combinations\n```\n:::\n\n### Tree model tuning with cross-validation\nCross-validation is required for tuning models. In order to find the best model, we can use resampled data from the training set to test the models against \"new data\" in order to see the variation within the model.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntree_res <- tree_wf %>% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control = control_grid(save_pred = TRUE)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n:::\n\nPlotting the resampled models produces a summary of the RMSE in the top row. A lower RMSE is preferred.\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nIt would appear that the model with tree_depth = 1 has the best RMSE.\n### Tree model performance\nTo confirm the best performing tree model, we can use `select_best()` function and extract the metrics.\n\n::: {.cell}\n\n```{.r .cell-code}\n#select best tree model\nbest_tree <- tree_res %>% select_best(\"rmse\")\nbest_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n```\n:::\n\n```{.r .cell-code}\n#set up workflow\ntreefinal_wf <- tree_wf %>% \n  finalize_workflow(best_tree)\n\n#fit the final model\ntreefinal_fit <- treefinal_wf %>% \n  fit(train_data1) \n\n#extract RMSE value\ntreefinalfitted_rmse<- augment(treefinal_fit, train_data1) %>% \n  select(BodyTemp, .pred) %>% \n  rmse(truth = BodyTemp, .pred)\n```\n:::\n\nThe best performing tree model has a tree depth of 1 and and RMSE of *1.178* which is slightly better than the training null model of *1.209*. \n\n::: {.cell}\n\n```{.r .cell-code}\n#save for later comparison: RMSE based on predictions from best model\ntree_rmse <- tree_res %>% \n  collect_predictions(parameters = best_tree) %>% \n  rmse(BodyTemp, .pred) %>% \n  mutate(model = \"Tree\")\ntree_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model\n  <chr>   <chr>          <dbl> <chr>\n1 rmse    standard        1.19 Tree \n```\n:::\n:::\n\nThe decision tree can be plotted using `rpart.plot()`. The level of importance for each predictor can be found using `vip()` function.\n\n::: {.cell}\n\n```{.r .cell-code}\n#extract engine and plot decision tree\ntreefinal_fit%>% \n  extract_fit_engine() %>% \n  rpart.plot(roundint = FALSE)\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#plot importance of predictors\ntreefinal_fit %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n### Plots for Tree model\nWe may also be interested in the predictions and residuals for the model compared to the actual outcomes. First, we need to create the predictions using `augment()` and the residuals using `mutate()`.\n\n::: {.cell}\n\n```{.r .cell-code}\n#create predictions and residuals\ntree_resd <- augment(treefinal_fit, train_data1) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n\n#plot body temperature and predictions\ntreeplot1 <- ggplot(tree_resd)+\n  geom_point(aes(BodyTemp, .pred))+\n  scale_x_continuous(limits = c(97,103.5))+\n  scale_y_continuous(limits = c(97,103.5))\ntreeplot1\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot body temperature and residuals\ntreeplot2 <- ggplot(tree_resd)+\n  geom_point(aes(BodyTemp,.resid))\ntreeplot2\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\nThe predictions and residuals show that the models do not perform well in predicting body temperature. \n\n### Compare model to null\n\nAs mentioned before, the tree model is only slightly better than the null model (RMSE of 1.88 (SE = 0.061) compared to 1.209). The fitted tree model has a RMSE of *1.178*.\n\n::: {.cell}\n\n```{.r .cell-code}\n#metrics for best model\ntree_res %>% show_best()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n1    0.0000000001          1 rmse    standard    1.19     5  0.0613 Preprocesso…\n2    0.0000000178          1 rmse    standard    1.19     5  0.0613 Preprocesso…\n3    0.00000316            1 rmse    standard    1.19     5  0.0613 Preprocesso…\n4    0.000562              1 rmse    standard    1.19     5  0.0613 Preprocesso…\n5    0.0000000001          4 rmse    standard    1.20     5  0.0504 Preprocesso…\n```\n:::\n\n```{.r .cell-code}\ntreefinalfitted_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.18\n```\n:::\n:::\n\n\n### LASSO model specifications\nNext we will look at the LASSO model following the same general steps as the decision tree model. \n\n::: {.cell}\n\n```{.r .cell-code}\n#set LASSO model\nlm_modLASSO <- linear_reg(penalty = tune(), #penalties given for number of predictors\n                          mixture = 1) %>% \n  set_engine(\"glmnet\")\n```\n:::\n\n\n### LASSO model workflow\n\n::: {.cell}\n\n```{.r .cell-code}\n#set LASSO workflow\nLASS_wf <- workflow() %>% \n  add_model(lm_modLASSO) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n\n### LASSO model grid specifications\nWe will use a similar grid method as before in order to tune the parameters of the models. \n\n::: {.cell}\n\n```{.r .cell-code}\n#create LASSO grid\nLASS_grid <-tibble(penalty = 10^seq(-4, -1, length.out = 30)) #creates a grid of penalty values to tune\nLASS_grid %>% top_n(-5) #lowest penalty values\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by penalty\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n```\n:::\n:::\n\nPenalties will be highest for models with the most predictors. \n\n### LASSO model tuning with cross-validation\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nLASS_res <-\n  LASS_wf %>% \n  tune_grid(folds, #cross-validation\n            grid = LASS_grid, #penalty grid\n            control = control_grid(save_pred = TRUE), #save CV predictions to use later\n            metrics = metric_set(rmse)) #RMSE as metric\n```\n:::\n\n\nWe can plot the metrics against penalty values.\n\n::: {.cell}\n\n```{.r .cell-code}\nLASS_plot <- \n  LASS_res %>% \n  autoplot()\n\nLASS_plot \n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\nWe would like the lowest RMSE and the lowest penalty, so the best model should have an RMSE around 1.15.\n\n### LASSO model performance\nUsing `select_best()` function and extracting the metrics will give us the model performance measure.\n\n::: {.cell}\n\n```{.r .cell-code}\n#show top performing models\nLASS_showbest <- LASS_res %>% show_best()\n\n#select best model\nbest_LASS_model <- LASS_res %>% \n  select_best(\"rmse\")\nbest_LASS_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0386 Preprocessor1_Model26\n```\n:::\n\n```{.r .cell-code}\n#update workflow with best model\nLASSfinal_wf <- LASS_wf %>% \n  finalize_workflow(best_LASS_model)\n\n#fit the model to the training data\nLASSfinal_fit <- LASSfinal_wf %>% \n  fit(train_data1) \n\n#collect final RMSE\nLASSfinalfitted_rmse<- augment(LASSfinal_fit, train_data1) %>% \n  select(BodyTemp, .pred) %>% \n  rmse(truth = BodyTemp, .pred)\n```\n:::\n\nThe LASSO model (*RMSE = 1.117*) performed better than the decision tree model (*1.187*) and the null model (*1.209*).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#save for later comparison: RMSE based on predictions from best model\nLASS_rmse <- LASS_res %>% \n  collect_predictions(parameters = best_LASS_model) %>% \n  rmse(BodyTemp, .pred) %>% \n  mutate(model = \"LASSO\")\nLASS_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model\n  <chr>   <chr>          <dbl> <chr>\n1 rmse    standard        1.15 LASSO\n```\n:::\n:::\n\n\n### Plots for LASSO model\nWe can again plot the predictions and residuals against the outcome. \n\n::: {.cell}\n\n```{.r .cell-code}\n#create predictions and residuals\nLASS_resd <- augment(LASSfinal_fit, train_data1) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plots LASSO predictions and body temperature\nLASS_plot1 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp, .pred))+\n  scale_x_continuous(limits = c(97,103.5))+\n  scale_y_continuous(limits = c(97,103.5))\nLASS_plot1\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plots residuals and body temperature\nLASS_plot2 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp,.resid))\nLASS_plot2\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\nThe prediction and residual plots show a better performance than the other two models we have looked at so far. \n\n### Compare model to null model\nAs mentioned, the LASSO model performs better than the null mode with a RMSE of *1.148 (SE = 0.0534)* compared the null model *(1.209)*. The fitted LASSO model has an RMSE of *1.117*.\n\n::: {.cell}\n\n```{.r .cell-code}\nLASS_res %>% \n  show_best() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0386 rmse    standard    1.15     5  0.0534 Preprocessor1_Model26\n2  0.0489 rmse    standard    1.15     5  0.0542 Preprocessor1_Model27\n3  0.0304 rmse    standard    1.15     5  0.0528 Preprocessor1_Model25\n4  0.0621 rmse    standard    1.15     5  0.0552 Preprocessor1_Model28\n5  0.0240 rmse    standard    1.15     5  0.0524 Preprocessor1_Model24\n```\n:::\n\n```{.r .cell-code}\nLASSfinalfitted_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.12\n```\n:::\n:::\n\n\n### Random forest model specifications\nFinally, following the same steps are above, we can look at random forest models using cross-validation and tuning to find the best model.\n\n::: {.cell}\n\n```{.r .cell-code}\n#how many cores your computer can use for running tuning on multiple models at the same time \ncores <- parallel::detectCores()\n\n#set up random forest model\nrf_mod <- rand_forest(mtry = tune(), #parameter to tune\n                      min_n = tune(), #parameter to tune\n                      trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% \n  set_mode(\"regression\")\n```\n:::\n\n\n### Random forest model workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create random forest recipe\nrf_rec <- recipe(BodyTemp ~ ., data = train_data1) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_ordinalscore()\n#create workflow\nrf_wf <- workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(rf_rec)\n```\n:::\n\n\n### Random forest model grid specifications/tuning with cross-validation\nWe will use a similar grid method for tuning model parameters\n\n::: {.cell}\n\n```{.r .cell-code}\n#create grid for tuning parameters for cross-validation\nset.seed(123)\nrf_res <- rf_wf %>% \n  tune_grid(folds,\n            grid = 25, #25 models\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(rmse))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n:::\n\n\nWe can look at the best random forest models by RMSE values. \n\n::: {.cell}\n\n```{.r .cell-code}\n#show top performing models\nrf_showbest <- rf_res %>% show_best()\n\nrf_showbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8    37 rmse    standard    1.16     5  0.0574 Preprocessor1_Model25\n2     4    14 rmse    standard    1.16     5  0.0589 Preprocessor1_Model06\n3     3    23 rmse    standard    1.16     5  0.0603 Preprocessor1_Model13\n4    13    35 rmse    standard    1.16     5  0.0564 Preprocessor1_Model15\n5     7    12 rmse    standard    1.17     5  0.0559 Preprocessor1_Model18\n```\n:::\n:::\n\nThe best performing model has mtry (number of predictors at each node) = 8, min_n (min number of data points required to split) = 37 and RMSE of *1.158 (SE = 0.057)*. This is a better performing model compared to the null and other models.\n\nThe `autoplot()` function visually shows the performance of the various models. \n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>% autoplot()\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\nWe will extract the best model from the group using `select_best()`.\n\n::: {.cell}\n\n```{.r .cell-code}\n#extract best model\nrf_best <-\n  rf_res %>% \n  select_best(metric = \"rmse\")\n#show results\nrf_best\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     8    37 Preprocessor1_Model25\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#collect predictions of best random forest model and calculate RMSE\n#save for later comparison: RMSE based on predictions from best model\nrf_rmse <-\n  rf_res %>% \n  collect_predictions(parameters = rf_best) %>% \n  rmse(truth = BodyTemp, estimate = .pred) %>% \n  mutate(model = \"Random Forest\")\nrf_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric .estimator .estimate model        \n  <chr>   <chr>          <dbl> <chr>        \n1 rmse    standard        1.16 Random Forest\n```\n:::\n:::\n\nUsing the best random forest model, the RMSE is *1.163* which is similar to the LASSO model.\n\n### Final fit for random forest model\nNow we can fit the model to the training data to get the final RMSE value.\n\n::: {.cell}\n\n```{.r .cell-code}\n#set random forest model using best model parameters\nrffinal_mod <-\n  rand_forest(mtry = 8, min_n = 37, trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") %>% \n  set_mode(\"regression\")\n#update workflow\nrffinal_wf <- rf_wf %>% \n  update_model(rffinal_mod)\n#fit to training data\nrffinal_fit <- rffinal_wf %>%\n  fit(train_data1)\n#pull RMSE from fitted model\nrffinalfitted_rmse <- augment(rffinal_fit, train_data1) %>% \n  select(BodyTemp, .pred) %>% \n  rmse(truth = BodyTemp, .pred)\n```\n:::\n\nThe fitted random forest model RMSE is *1.028* which is slightly better than the LASSO model and better than the null and decision tree models. \n\nWe can also look at the important predictors for the random forest model using `vip()` function`\n\n::: {.cell}\n\n```{.r .cell-code}\nrffinal_fit %>% \n  extract_fit_parsnip() %>% \n  vip(num_features = 20)\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\nSneezing remains as the most important predictor for body temperature followed closely by subjective fever.\n\n### Plots for Random forest model\nWe will again plot the predictions and residuals for the model using `augment()`.\n\n::: {.cell}\n\n```{.r .cell-code}\n#create predictions and residuals\nrf_resd <- augment(rffinal_fit, train_data1) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n\n#plot body temperature and predictions\nrf_plot1 <- ggplot(rf_resd)+\n  geom_point(aes(BodyTemp,.pred))\nrf_plot1\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot body temperature and residuals\nrf_plot2 <- ggplot(rf_resd)+\n  geom_point(aes(BodyTemp, .resid))\nrf_plot2\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\nThe residuals and prediction plots look similar to the LASSO model.\n\n### Compare model to null model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>% show_best() %>% arrange(mean) %>% top_n(1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by .config\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8    37 rmse    standard    1.16     5  0.0574 Preprocessor1_Model25\n```\n:::\n:::\n\nCompared to the null, the random forest model is a better predictor of body temperature with a *RMSE of 1.158 (SE = 0.057)* compared to *1.209*.\n\n## Choosing best model\nNow that we have looked at all of the models, we can choose the best model to test with our `test_data1` data set which has not been used during the tuning/fitting process. \n\n::: {.cell}\n\n```{.r .cell-code}\n#create comparison tibble for three models\ncompare <- bind_rows(tree_rmse,LASS_rmse,rf_rmse)\ncompare\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  .metric .estimator .estimate model        \n  <chr>   <chr>          <dbl> <chr>        \n1 rmse    standard        1.19 Tree         \n2 rmse    standard        1.15 LASSO        \n3 rmse    standard        1.16 Random Forest\n```\n:::\n:::\n\nBased on the RMSE metric, the best performing models are the LASSO and Random forest models. We can also check the uncertainty by looking at the plots shown above and the standard error. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nLASS_showbest %>% head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0386 rmse    standard    1.15     5  0.0534 Preprocessor1_Model26\n```\n:::\n\n```{.r .cell-code}\nrf_showbest %>% head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8    37 rmse    standard    1.16     5  0.0574 Preprocessor1_Model25\n```\n:::\n:::\n\nSince the LASSO model has a lower SE than the random forest model, we will choose the LASSO model for testing the model on new data.\n\n## Final evaluation\nNow we can fit the LASSO model to the test data to check the RMSE on \"new data\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLASStest_fit <-\n  LASSfinal_wf %>% \n  last_fit(data_split1)\nLASStest_fit %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      1.16   Preprocessor1_Model1\n2 rsq     standard      0.0297 Preprocessor1_Model1\n```\n:::\n:::\n\nWith the test data, the model has a RMSE of *1.16* which is slightly worse than the fitted model. This would make sense as the fitted model is designed off the `train_data1` set. \n\nWe will plot the same prediction and residual plots as before using the `test_data1` outcomes. They will look pretty similar to the fitted LASSO model since the RMSE is similar.\n\n::: {.cell}\n\n```{.r .cell-code}\n#create predictions and residuals for test data\nLASStest_resd <- augment(LASStest_fit) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plots LASSO predictions and body temperature for test data\nLASS_plot3 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp, .pred))+\n  scale_x_continuous(limits = c(97,103.5))+\n  scale_y_continuous(limits = c(97,103.5))\nLASS_plot3\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plots residuals and body temperature for test data\nLASS_plot4 <- ggplot(LASS_resd)+\n  geom_point(aes(BodyTemp,.resid))\nLASS_plot4\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "machinelearning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}