{
  "hash": "94d7625b1bf5425d31b00d9f06f1b282",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning\"\noutput: html_document\n---\n\n\nFor the machine learning exercise, we will use the `flu` data and focus on Body Temperature as the outcome. \n\n## Load packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(here)\nlibrary(rsample)\nlibrary(rpart.plot)\nlibrary(vip)\nlibrary(glmnet)\nlibrary(ranger)\n```\n:::\n\n\n## Load data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load data\nfilelocation <- here(\"fluanalysis\", \"data\", \"cleandata.rds\")\nload(filelocation)\n#reassign as flu\nflu <- cleandata\n\nflu <- flu %>% \n  select(!c(\"Vision\",\"Hearing\")) #remove columns with <50 entries in one category\n```\n:::\n\n\n## Data splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#establish reproducibility by setting the seed\nset.seed(123)\n\n#add data to the training set\ndata_split1 <- initial_split(flu, prop = 7/10, strata = BodyTemp)\n\n#create two data sets with 70/30% of data in training set\ntrain_data1 <- training(data_split1)\ntest_data1 <- testing(data_split1)\n```\n:::\n\n\n## Cross Validation & Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- vfold_cv(train_data1, v = 5)\n\nml_rec <- recipe(BodyTemp ~., data = train_data1) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_ordinalscore()\n\nlm_mod <- linear_reg() \n\nml_wf <- workflow() %>% \n  add_model(lm_mod) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n\n## Null model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnullmod <- null_model() %>% \n  set_engine(\"parsnip\") %>% \n  set_mode(\"regression\") %>% \n  translate()\n\nnullmodwf <- workflow() %>% \n  add_model(nullmod) %>% \n  add_recipe(ml_rec)\n\nnullmodfit <- nullmodwf %>% \n  fit(data = train_data1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnullmod_predtrain <-augment(nullmodfit, train_data1) %>% \n  rmse(truth = BodyTemp, .pred)\nnullmod_predtrain\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.21\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnullmod_predtest <-augment(nullmodfit, test_data1) %>% \n  rmse(truth = BodyTemp, .pred)\nnullmod_predtest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.16\n```\n:::\n:::\n\n\n\n## Model tuning and fitting\n\n### Tree model specification\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_spec <- decision_tree(\n  cost_complexity = tune(),\n  tree_depth = tune()\n) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n```\n:::\n\n### Tree model workflow\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_wf <- workflow() %>% \n  add_model(tune_spec) %>% \n  add_recipe(ml_rec)\n```\n:::\n\n### Tree model grid specification\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n```\n:::\n\n### Tree model tuning with cross-validation\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res <- tree_wf %>% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5: internal: A correlation computation is required, but `estimate` is constant and ha...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tree <- tree_res %>% select_best(\"rmse\")\nbest_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n```\n:::\n\n```{.r .cell-code}\ntreefinal_wf <- tree_wf %>% \n  finalize_workflow(best_tree)\n\ntreefinal_fit <- treefinal_wf %>% \n  last_fit(data_split1) #fits for test data\n\ntreefinal_fit %>% collect_metrics() #double check, higher than nullmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard    1.19     Preprocessor1_Model1\n2 rsq     standard    0.000889 Preprocessor1_Model1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_tree <- extract_workflow(treefinal_fit)\n\nfinal_tree %>% \n  extract_fit_engine() %>% \n  rpart.plot(roundint = FALSE)\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_tree %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n### Plots for Tree model - test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_test_resd <- augment(treefinal_fit) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntesttreeplot1 <- ggplot(tree_test_resd)+\n  geom_point(aes(BodyTemp, .pred))\ntesttreeplot1\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntesttreeplot2 <- ggplot(tree_test_resd)+\n  geom_point(aes(BodyTemp,.resid))\ntesttreeplot2\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n### Tree modeling with Training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntreefinal_fit2 <- treefinal_wf %>% \n  fit(data = train_data1) #fits for test data\n\ntreefinal_fitaug <- augment(treefinal_fit2, train_data1) %>%\n  rmse(truth = BodyTemp, .pred)\ntreefinal_fitaug\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.18\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntreefinal_fit2 %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n### Plots for Tree model - test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_train_resd <- augment(treefinal_fit2, train_data1) %>% \n  select(c(.pred, BodyTemp)) %>% \n  mutate(.resid = BodyTemp - .pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntraintreeplot1 <- ggplot(tree_train_resd)+\n  geom_point(aes(BodyTemp, .pred))\ntraintreeplot1\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntraintreeplot2 <- ggplot(tree_train_resd)+\n  geom_point(aes(BodyTemp,.resid))\ntraintreeplot2\n```\n\n::: {.cell-output-display}\n![](machinelearning_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n### LASSO model\n\n\n### Random forest model",
    "supporting": [
      "machinelearning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}